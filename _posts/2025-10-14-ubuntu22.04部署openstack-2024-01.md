---
layout    : post
title     : "ubuntu22.04部署openstack-2024.01"
date      : 2025-04-23
lastupdate: 2025-10-14
categories: [Openstack]
render_with_liquid: false
---

# ubuntu22.04部署openstack-2024.01



| 节点       | 管理网络接口 | 外部网络接口  | 磁盘              |
| ---------- | ------------ | ------------- | ----------------- |
| controller | 10.0.0.10    | 192.168.35.10 | sda 100G          |
| compute01  | 10.0.0.20    | 192.168.35.20 | sda 100G sdb 100G |
| compute02  | 10.0.0.30    | 192.168.35.30 | sda 100G sdb 100G |
| compute02  | 10.0.0.40    | 192.168.35.40 | sda 100G sdb 100G |



## **基础环境配置**



### **时间同步（chrony）**



#### **controller node**

安装chrony服务

```bash
apt install chrony -y
```

编辑` /etc/chrony/chrony.conf`文件：

```bash
# 使用controller节点的本地时间进行时间同步
server controller iburst
# 允许10.0.0.0/24 网段通过
allow 10.0.0.0/24
```

重启服务以生效

```bash
service chrony restart
```



#### compute node

安装chrony服务

```bash
apt install chrony -y
```

编辑` /etc/chrony/chrony.conf`文件：

```bash
# 同步controller节点时间
server controller iburst
```

重启服务以生效

```bash
service chrony restart
```



### 启用openstack归档文件

这里采用的是**适用于 Ubuntu 2024.1 LTS 的 OpenStack 22.04 Caracal**



**every node**

```bash
add-apt-repository cloud-archive:caracal
```

安装openstack客户端工具

```bash
apt install python3-openstackclient
```



### 安装sql数据库



**controller node**

安装和配置组件

1. 安装软件包：

   ```bash
   apt install mariadb-server python3-pymysql -y
   ```

2. 创建并编辑 `/etc/mysql/mariadb.conf.d/99-openstack.cnf` 文件并完成以下操作：

- 创建一个 `[mysqld]` 部分，并设置 `bind-address` key 添加到控制器节点的管理 IP 地址 允许其他节点通过管理网络进行访问。设置 用于启用有用选项的附加键和 UTF-8 字符集：

  ```bash
  # 设置成控制节点IP，仅允许内网访问
  [mysqld]
  bind-address = 10.0.0.10
  
  default-storage-engine = innodb
  innodb_file_per_table = on
  max_connections = 4096
  collation-server = utf8_general_ci
  character-set-server = utf8
  ```

> **注意：**如果使用内网IP,需要做以下操作,否则后续同步数据库会遇到问题

编辑`/etc/mysql/mariadb.conf.d/50-server.cnf`文件:

将`bind-address`参数的值改为内网IP

```bash
cat /etc/mysql/mariadb.conf.d/50-server.cnf | grep bind-address
bind-address            = 10.0.0.10
```



完成安装

1. 重新启动数据库服务：

   ```bash
   service mysql restart
   ```

2. 通过运行 `mysql_secure_installation` 来保护数据库服务 脚本。特别是，为数据库选择合适的密码 `根`账户：

   ```bash
   mysql_secure_installation
   ```





### 消息队列（RabbitMQ）



**controller node**

安装和配置组件

1. 安装软件包：

   ```bash
   apt install rabbitmq-server -y
   ```

2. 添加 `openstack` 用户：

   ```bash
   # rabbitmqctl add_user openstack 000000
   Adding user "openstack" ...
   ```

3. 允许 `OpenStack` 用户：

   ```bash
   # rabbitmqctl set_permissions openstack ".*" ".*" ".*"
   Setting permissions for user "openstack" in vhost "/" ...
   ```



### Memcached



**controller node**

安装和配置组件

1. 安装软件包：

   ```bash
   apt install memcached python3-memcache -y
   ```

2. 编辑 `/etc/memcached.conf` 文件，并将服务配置为使用控制器节点的管理 IP 地址。这是为了允许其他节点通过管理网络进行访问：

   ```bash
   -l 10.0.0.10
   ```

> **注意:** 更改包含 `-l 127.0.0.1` 的现有行。

完成安装

重新启动 Memcached 服务：

```bash
service memcached restart
```





### Etcd



**controller node**

安装和配置组件

1. 安装 `etcd` 包：

   ```bash
   apt install etcd-server -y
   ```

2. 编辑 `/etc/default/etcd` 文件并设置 `ETCD_INITIAL_CLUSTER`。具体如下：

   ```bash
   ETCD_NAME="controller"
   ETCD_DATA_DIR="/var/lib/etcd"
   ETCD_INITIAL_CLUSTER_STATE="new"
   ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"
   ETCD_INITIAL_CLUSTER="controller=http://10.0.0.10:2380"
   ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.0.0.10:2380"
   ETCD_ADVERTISE_CLIENT_URLS="http://10.0.0.10:2379"
   ETCD_LISTEN_PEER_URLS="http://0.0.0.0:2380"
   ETCD_LISTEN_CLIENT_URLS="http://10.0.0.10:2379"
   ```

完成安装

1. 启用并重启 etcd 服务：

   ```bash
   systemctl enable etcd
   systemctl restart etcd
   ```



## 安装openstack服务

| 服务      | 密码    |
| --------- | ------- |
| keystone  | 000000  |
| glance    | 000000  |
| placement | 000000  |
| nova      | 000000  |
| neutron   | 000000  |
| dashboard | 000000  |
| cinder    | 000000  |
| swift     | 000000  |
| skyline   | skyline |
| octavia   | 000000  |



### Keystone



**controller node**



#### 先决条件

在安装和配置 Identity 服务之前，您必须创建一个数据库。

1. 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

   ```bash
   mysql -uroot -p
   ```

2. 创建 `keystone` 数据库：

   ```bash
   MariaDB [(none)]> CREATE DATABASE keystone;
   ```

3. 授予对 `keystone` 数据库的适当访问权限：

   ```bash
   MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' \
   IDENTIFIED BY '000000';
   MariaDB [(none)]> GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' \
   IDENTIFIED BY '000000';
   ```

将 `KEYSTONE_DBPASS` 替换为合适的密码。

4. 退出数据库访问客户端。



#### 安装和配置组件

1. 执行以下命令安装软件包。

   ```bash
   apt install keystone -y
   ```

2. 编辑 `/etc/keystone/keystone.conf` 文件并完成以下作：

   - 在 `[database]` 部分中，配置数据库访问：

     ```bash
     [database]
     # ...
     connection = mysql+pymysql://keystone:000000@controller/keystone
     ```

     将 KEYSTONE_DBPASS 替换为您为数据库选择的密码。

   - 在 `[token]` 部分中，配置 Fernet 令牌提供程序：

     ```bash
     [token]
     # ...
     provider = fernet
     ```

3. 填充 Identity 服务数据库：

   ```bash
   su -s /bin/sh -c "keystone-manage db_sync" keystone
   ```

   > **注意：**`--keystone-user` 和 `--keystone-group` 标志用于指定将用于运行 keystone 的作系统用户/组。提供这些是为了允许在另一个作系统用户/组下运行 keystone。在下面的示例中，我们称之为用户和组 `keystone`。

4. 初始化 Fernet 密钥存储库：

   ```bash
   keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
   keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
   ```

5. 引导 Identity 服务：

   ```bash
   keystone-manage bootstrap --bootstrap-password 000000 \
     --bootstrap-admin-url http://controller:5000/v3/ \
     --bootstrap-internal-url http://controller:5000/v3/ \
     --bootstrap-public-url http://controller:5000/v3/ \
     --bootstrap-region-id RegionOne
   ```

   将 `ADMIN_PASS` 替换为适合管理用户的密码。

创建一个服务项目，该项目包含您添加到环境中的每个服务的唯一用户。创建`service` 项目：

```bash
openstack project create --domain default --description "Service Project" service
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | Service Project                  |
| domain_id   | default                          |
| enabled     | True                             |
| id          | 81dea4257b404d27a7c021e1519c633d |
| is_domain   | False                            |
| name        | service                          |
| options     | {}                               |
| parent_id   | default                          |
| tags        | []                               |
+-------------+----------------------------------+
```



#### **配置 Apache HTTP 服务器**

1. 编辑 `/etc/apache2/apache2.conf` 文件并配置 `ServerName` 选项来引用控制器节点：

   ```bash
   ServerName controller
   ```

   如果 `ServerName` 条目尚不存在，则需要添加该条目。

#### 完成安装

1. 重新启动 Apache 服务：

   ```bash
   service apache2 restart
   ```

2. 通过设置适当的环境变量来配置管理帐户：

   ```bash
   cat > /etc/keystone/admin-openrc.sh << EOF
   export OS_USERNAME=admin
   export OS_PASSWORD=000000
   export OS_PROJECT_NAME=admin
   export OS_USER_DOMAIN_NAME=Default
   export OS_PROJECT_DOMAIN_NAME=Default
   export OS_AUTH_URL=http://controller:5000/v3
   export OS_IDENTITY_API_VERSION=3
   EOF
   ```

   将 `ADMIN_PASS` 替换为管理用户的密码。

**创建域、项目、用户和角色**





### Glance



**controller node**

#### 先决条件

在安装和配置 Image 服务之前，您必须创建数据库、服务凭证和 API 终端节点。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

     ```bash
     mysql
     ```

   - 创建 `glance` 数据库：

     ```bash
     MariaDB [(none)]> CREATE DATABASE glance;
     ```

   - 授予对 `glance` 数据库的适当访问权限：

     ```bash
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
       IDENTIFIED BY '000000';
     ```

     将 `GLANCE_DBPASS` 替换为合适的密码。

   - 退出数据库访问客户端。

2. 获取`管理员`凭证以获取仅限管理员的 CLI 命令的访问权限：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 要创建服务凭证，请完成以下步骤：

   - 创建 `glance` 用户：

     ```bash
     openstack user create --domain default --password-prompt glance
     ```

   - 将 `admin` 角色添加到 `glance` 用户，然后 `service`项目：

     ```bash
     openstack role add --project service --user glance admin
     ```

   - 创建 `glance` 服务实体：

     ```bash
     openstack service create --name glance \
       --description "OpenStack Image" image
     ```

4. 创建图像服务 API 终端节点：

   ```bash
   openstack endpoint create --region RegionOne \
     image public http://controller:9292
     
   openstack endpoint create --region RegionOne \
     image internal http://controller:9292
     
   openstack endpoint create --region RegionOne \
     image admin http://controller:9292
   ```

5. 注册配额限制（可选）：

   如果您决定在 Glance 中使用每租户配额，则必须先在 Keystone 中注册限制：

   ```bash
   openstack --os-cloud devstack-system-admin registered limit create \
     --service glance --default-limit 1000 --region RegionOne image_size_total
     
   openstack --os-cloud devstack-system-admin registered limit create \
     --service glance --default-limit 1000 --region RegionOne image_stage_total
     
   openstack --os-cloud devstack-system-admin registered limit create \
     --service glance --default-limit 100 --region RegionOne image_count_total
     
   openstack --os-cloud devstack-system-admin registered limit create \
     --service glance --default-limit 100 --region RegionOne \
     image_count_uploading
   ```

   请务必在 `glance-api.conf`文件中设置 `use_keystone_limits=True`。



#### 安装和配置组件

1. 安装软件包：

   ```bash
   apt install glance -y
   ```

2. 编辑 `/etc/glance/glance-api.conf` 文件并完成以下作：

   - 在 `[database]` 部分中，配置数据库访问：

     ```bash
     [database]
     # ...
     connection = mysql+pymysql://glance:000000@controller/glance
     ```

     将 `GLANCE_DBPASS` 替换为您为 Image 服务数据库选择的密码。

   - 在 `[keystone_authtoken]` 和 `[paste_deploy]` 部分中，配置 Identity Service 访问：

     ```bash
     [keystone_authtoken]
     # ...
     www_authenticate_uri  = http://controller:5000
     auth_url = http://controller:5000
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = Default
     user_domain_name = Default
     project_name = service
     username = glance
     password = 000000
     
     [paste_deploy]
     # ...
     flavor = keystone
     ```

     将 `GLANCE_PASS` 替换为您为 `glance` 用户设置的密码。

     > **注意:** 注释掉或删除 `[keystone_authtoken]` 部分。

   - 在 `[glance_store]` 部分中，配置本地文件系统存储和图像文件的位置：

     ```bash
     [DEFAULT]
     # ...
     enabled_backends=fs:file
     
     [glance_store]
     # ...
     default_backend = fs
     
     // [fs]新建
     [fs]
     # ...
     filesystem_store_datadir = /var/lib/glance/images/
     ```

   - 在 `[oslo_limit]` 部分中，配置对 keystone 的访问: 

     ```bash
     [oslo_limit]
     auth_url = http://controller:5000
     auth_type = password
     user_domain_id = default
     username = glance
     system_scope = all
     password = GLANCE_PASS
     endpoint_id = 340be3625e9b4239a6415d034e98aace
     region_name = RegionOne
     ```

     将`endpoint_id` 替换为 之前给`image`创建的`public`的`endpoint`

     将 `GLANCE_PASS` 替换为您为 `glance` 用户设置的密码。

     确保 glance 账户对系统范围的资源（如限制）具有读取者访问权限：

     ```bash
     openstack role add --user glance --user-domain Default --system all reader
     ```

   - 在 `[DEFAULT]` 部分中，可以选择启用每租户配额：

     ```bash
     [DEFAULT]
     use_keystone_limits = True
     ```

     请注意，如果启用此功能，则必须如上所述创建注册限制。

3. 填充 Image 服务数据库：

   ```bash
   su -s /bin/sh -c "glance-manage db_sync" glance
   ...
   Database is synced successfully.
   ```



#### 完成安装

1. 重新启动 Image 服务：

   ```bash
   service glance-api restart
   ```

#### 验证

1. 获取`管理员`凭证以获取仅限管理员的 CLI 命令的访问权限：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

2. 下载源图像(linux下载缓慢,可以使用浏览器下载)：

   ```bash
   wget https://download.cirros-cloud.net/0.6.2/cirros-0.6.2-x86_64-disk.img
   ```

3. 使用 [QCOW2](https://docs.openstack.org/glance/2024.1/glossary.html#term-QEMU-Copy-On-Write-2-QCOW2) 磁盘格式，[ bare](https://docs.openstack.org/glance/2024.1/glossary.html#term-bare) 容器格式和公共可见性，以便所有项目都可以访问它：

   ```bash
   glance image-create --name "cirros" \
     --file /opt/cirros-0.6.2-x86_64-disk.img \
     --disk-format qcow2 --container-format bare \
     --visibility=public
   ```

4. 确认上传图像并验证属性：

   ```bash
   glance image-list
   +--------------------------------------+--------+
   | ID                                   | Name   |
   +--------------------------------------+--------+
   | f1c973a4-d5ea-4ea1-9ecf-48945053a655 | cirros |
   +--------------------------------------+--------+
   ```





### Placement



#### 先决条件

在安装和配置置放服务之前，您必须创建数据库、服务凭证和 API 终端节点。

创建数据库 

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

     ```bash
     mysql -uroot -p000000
     ```

   - 创建`placement`数据库：

     ```bash
     MariaDB [(none)]> CREATE DATABASE placement;
     ```

   - 授予对数据库的适当访问权限：

     ```bash
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' \
       IDENTIFIED BY '000000';
     ```

     将 `PLACEMENT_DBPASS` 替换为合适的密码。

   - 退出数据库访问客户端。

配置用户和端点

1. 获取`管理员`凭证以获取仅限管理员的 CLI 命令的访问权限：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

2. 使用您选择的 Placement service 用户`创建 PLACEMENT_PASS`：

   ```bash
   openstack user create --domain default --password-prompt placement
   User Password:
   Repeat User Password:
   +---------------------+----------------------------------+
   | Field               | Value                            |
   +---------------------+----------------------------------+
   | domain_id           | default                          |
   | enabled             | True                             |
   | id                  | 6e6ee4b18ea245faaadda72ffa150176 |
   | name                | placement                        |
   | options             | {}                               |
   | password_expires_at | None                             |
   +---------------------+----------------------------------+
   ```

3. 将 Placement 用户添加到具有 admin 角色的服务项目中：

   ```bash
   openstack role add --project service --user placement admin
   ```

4. 在服务目录中创建 Placement API 条目：

   ```bash
   openstack service create --name placement \
     --description "Placement API" placement
   ```

5. 创建 Placement API 服务终端节点：

   ```bash
   openstack endpoint create --region RegionOne \
     placement public http://controller:8778
     
   openstack endpoint create --region RegionOne \
     placement internal http://controller:8778
   
   openstack endpoint create --region RegionOne \
     placement admin http://controller:8778
   ```

#### 安装和配置组件

1. 安装软件包：

   ```bash
   apt install placement-api -y
   ```

2. 编辑 `/etc/placement/placement.conf` 文件并完成以下作：

   - 在 `[placement_database]` 部分中，配置数据库访问：

     ```bash
     [placement_database]
     # ...
     connection = mysql+pymysql://placement:000000@controller/placement
     ```

   - 在 `[api]` 和 `[keystone_authtoken]` 部分中，配置 Identity 服务访问：

     ```bash
     [api]
     # ...
     auth_strategy = keystone
     
     [keystone_authtoken]
     # ...
     auth_url = http://controller:5000/v3
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = Default
     user_domain_name = Default
     project_name = service
     username = placement
     password = 000000
     ```

     将 `PLACEMENT_PASS` 替换为您为 `Identity` 服务中的 placement 用户。

3. 填充 `placement` 数据库：

   ```bash
   su -s /bin/sh -c "placement-manage db sync" placement
   ```

#### 完成安装

- 重新加载 Web 服务器以进行调整，以获取新的放置配置设置。

  ```bash
  service apache2 restart
  ```





### Nova



#### controller node

安装和配置控制器节点

##### 先决条件

在安装和配置 Compute 服务之前，您必须创建数据库、服务凭证和 API 终端节点。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

     ```bash
     mysql
     ```

   - 创建 `nova_api`、`nova` 和 `nova_cell0` 数据库：

     ```bash
     MariaDB [(none)]> CREATE DATABASE nova_api;
     MariaDB [(none)]> CREATE DATABASE nova;
     MariaDB [(none)]> CREATE DATABASE nova_cell0;
     ```

   - 授予对数据库的适当访问权限：

     ```bash
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \
       IDENTIFIED BY '000000';
     
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
       IDENTIFIED BY '000000';
     
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \
       IDENTIFIED BY '000000';
     ```

     将 `NOVA_DBPASS` 替换为合适的密码。

   - 退出数据库访问客户端。

2. 获取`管理员`凭证以获取仅限管理员的 CLI 命令的访问权限：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 创建 Compute 服务凭证：

   - 创建 `nova` 用户：

     ```bash
     openstack user create --domain default --password-prompt nova
     ```

   - 将 `admin` 角色添加到 `nova` 用户：

     ```bash
     openstack role add --project service --user nova admin
     ```

   - 创建 nova 服务实体：

     ```bash
     openstack service create --name nova \
       --description "OpenStack Compute" compute
     ```

4. 创建 Compute API 服务终端节点：

   ```bash
   openstack endpoint create --region RegionOne \
     compute public http://controller:8774/v2.1
   
   openstack endpoint create --region RegionOne \
     compute internal http://controller:8774/v2.1
   
   openstack endpoint create --region RegionOne \
     compute admin http://controller:8774/v2.1
   ```

5. 安装 Placement 服务并配置用户和终端节点：

##### 安装和配置组件

1.  安装软件包：

   ```bash
   apt install nova-api nova-conductor nova-novncproxy nova-scheduler -y
   ```

2. 编辑 `/etc/nova/nova.conf` 文件并完成以下作：

   - 在 `[api_database]` 和 `[database]` 部分中，配置数据库访问：

     ```bash
     [api_database]
     # ...
     connection = mysql+pymysql://nova:000000@controller/nova_api
     
     [database]
     # ...
     connection = mysql+pymysql://nova:000000@controller/nova
     ```

     将 `NOVA_DBPASS` 替换为您为 Compute 数据库选择的密码。

     

   - 在 `[DEFAULT]` 部分中，配置 `RabbitMQ` 消息队列访问：

     ```bash
     [DEFAULT]
     # ...
     transport_url = rabbit://openstack:000000@controller:5672/
     ```

     将 `RABBIT_PASS` 替换为您为 `openstack` 选择的密码 account 的 `RabbitMQ` 中。

     

   - 在 `[api]` 和 `[keystone_authtoken]` 部分中，配置 Identity 服务访问：

     ```bash
     [api]
     # ...
     auth_strategy = keystone
     
     [keystone_authtoken]
     # ...
     www_authenticate_uri = http://controller:5000/
     auth_url = http://controller:5000/
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = Default
     user_domain_name = Default
     project_name = service
     username = nova
     password = 000000
     ```

     将 `NOVA_PASS` 替换为您在 Identity 服务中为 `nova` 用户选择的密码。

   - 在 `[service_user]` 部分中，配置[服务用户令牌 ](https://docs.openstack.org/nova/2024.1/admin/configuration/service-user-token.html#service-user-token)：

     **测试报告：`auth_url`为`https://controller/identity`时，无法通过，应改为`auth_url = http://controller:5000/v3`**

     ```bash
     [service_user]
     send_service_user_token = true
     auth_url = http://controller:5000/v3
     auth_strategy = keystone
     auth_type = password
     project_domain_name = Default
     project_name = service
     user_domain_name = Default
     username = nova
     password = 000000
     ```

     将 `NOVA_PASS` 替换为您在 Identity 服务中为 `nova` 用户选择的密码。

   - 在 `[DEFAULT]` 部分中，配置 `my_ip` 选项以使用控制器节点的管理接口 IP 地址：

     ```bash
     [DEFAULT]
     # ...
     my_ip = 10.0.0.10
     ```

   - 配置 **/etc/nova/nova.conf** 的 `[neutron]` 部分。请参阅 [网络服务安装指南](https://docs.openstack.org/neutron/2024.1/install/controller-install-ubuntu.html#configure-the-compute-service-to-use-the-networking-service) 了解更多信息。

   

   - 在 `[vnc]` 部分中，将 VNC 代理配置为使用控制器节点的管理接口 IP 地址：

     ```bash
     [vnc]
     enabled = true
     # ...
     server_listen = $my_ip
     server_proxyclient_address = $my_ip
     ```

   - 在 `[glance]` 部分，配置图片服务 API 的位置：

     ```bash
     [glance]
     # ...
     api_servers = http://controller:9292
     ```

   - 在 `[oslo_concurrency]` 部分中，配置锁定路径：

     ```bash
     [oslo_concurrency]
     # ...
     lock_path = /var/lib/nova/tmp
     ```

   - **由于包装错误，请从[DEFAULT]部分移除log_dir选项。**

   - 在 `[placement]` 部分中，配置对 Placement 服务的访问权限：

     ```bash
     [placement]
     # ...
     region_name = RegionOne
     project_domain_name = Default
     project_name = service
     auth_type = password
     user_domain_name = Default
     auth_url = http://controller:5000/v3
     username = placement
     password = 000000
     ```

     将 `PLACEMENT_PASS` 替换为您为 `安装`时创建的 Placement Service 用户 [放置 ](https://docs.openstack.org/placement/2024.1/install/)。注释掉或删除 `[placement]` 部分中的任何其他选项。

3. 填充 `nova-api` 数据库：

   ```bash
   su -s /bin/sh -c "nova-manage api_db sync" nova
   ```

4. 注册 `cell0` 数据库：

   ```bash
   su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
   ```

5. 创建 `cell1` 单元格：

   ```bash
   su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova
   ```

6. 填充 nova 数据库：

   ```bash
   su -s /bin/sh -c "nova-manage db sync" nova
   ```

7. 验证 nova cell0 和 cell1 是否已正确注册：

   ```bash
   su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova
   +-------+--------------------------------------+------------------------------------------+-------------------------------------------------+----------+
   |  Name |                 UUID                 |              Transport URL               |               Database Connection               | Disabled |
   +-------+--------------------------------------+------------------------------------------+-------------------------------------------------+----------+
   | cell0 | 00000000-0000-0000-0000-000000000000 |                  none:/                  | mysql+pymysql://nova:****@controller/nova_cell0 |  False   |
   | cell1 | 526b214d-ee4a-4bf1-a3e3-1bfb22dbc06f | rabbit://openstack:****@controller:5672/ |    mysql+pymysql://nova:****@controller/nova    |  False   |
   +-------+--------------------------------------+------------------------------------------+-------------------------------------------------+----------+
   ```


##### 完成安装

- 重新启动 Compute 服务：

  ```bash
  service nova-api restart
  service nova-scheduler restart
  service nova-conductor restart
  service nova-novncproxy restart
  ```



#### compute node

##### 安装和配置组件

1. 安装软件包：

   ```bash
   apt install nova-compute -y
   ```

2. 编辑 `/etc/nova/nova.conf` 文件并完成以下作：

   - 在 `[DEFAULT]` 部分中，配置 `RabbitMQ` 消息队列访问：

     ```bash
     [DEFAULT]
     # ...
     transport_url = rabbit://openstack:000000@controller:5672/
     ```

     将 `RABBIT_PASS` 替换为您为 `openstack` 选择的密码 account 的 `RabbitMQ` 中。

   - 在 `[api]` 和 `[keystone_authtoken]` 部分中，配置 Identity 服务访问：

     ```bash
     [api]
     # ...
     auth_strategy = keystone
     
     [keystone_authtoken]
     # ...
     www_authenticate_uri = http://controller:5000/
     auth_url = http://controller:5000/
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = Default
     user_domain_name = Default
     project_name = service
     username = nova
     password = 000000
     ```

   - 在 `[service_user]` 部分中，配置[服务用户令牌 ](https://docs.openstack.org/nova/2024.1/admin/configuration/service-user-token.html#service-user-token)：

     **测试报告：`auth_url`为`https://controller/identity`时，无法通过，应改为`auth_url = http://controller:5000/v3`**
   
     ```bash
     [service_user]
     send_service_user_token = true
     auth_url = http://controller:5000/v3
     auth_strategy = keystone
     auth_type = password
     project_domain_name = Default
     project_name = service
     user_domain_name = Default
     username = nova
     password = 000000
     ```

     将 `NOVA_PASS` 替换为您在 Identity 服务中为 `nova` 用户选择的密码。

   - 在 `[DEFAULT]` 部分中，配置 `my_ip` 选项：
   
     ```bash
     [DEFAULT]
     # ...
     my_ip = 10.0.0.20
     ```

     替换为当前计算节点上管理网络接口的 IP 地址, 表格中的10.0.0.(20,30)

   - 配置 **/etc/nova/nova.conf** 的 `[neutron]` 部分。请参阅 [网络服务安装指南](https://docs.openstack.org/neutron/2024.1/install/compute-install-ubuntu.html#configure-the-compute-service-to-use-the-networking-service) 了解更多详情。

   

   - 在 `[vnc]` 部分中，启用并配置远程控制台访问：
   
     **注意：如果windows没有映射controller的IP，请将`novncproxy_base_url`参数中的controller改为实际IP**
   
     ```bash
     [vnc]
     # ...
     enabled = true
     server_listen = 0.0.0.0
     server_proxyclient_address = $my_ip
     novncproxy_base_url = http://10.0.0.10:6080/vnc_auto.html
     # novncproxy_base_url = http://controller:6080/vnc_auto.html 
     ```
   
     服务器组件侦听所有 IP 地址，而代理组件仅侦听计算节点的管理接口 IP 地址。基 URL 指示您可以使用 Web 浏览器访问此计算节点上实例的远程控制台的位置。
   
   - 在 `[glance]` 部分，配置图片服务 API 的位置：
   
     ```bash
     [glance]
     # ...
     api_servers = http://controller:9292
     ```
   
   - 在 `[oslo_concurrency]` 部分中，配置锁定路径：
   
     ```bash
     [oslo_concurrency]
     # ...
     lock_path = /var/lib/nova/tmp
     ```
   
   - 在 `[placement]` 部分中，配置 Placement API：
   
     ```bash
     [placement]
     # ...
     region_name = RegionOne
     project_domain_name = Default
     project_name = service
     auth_type = password
     user_domain_name = Default
     auth_url = http://controller:5000/v3
     username = placement
     password = 000000
     ```
   
     将 `PLACEMENT_PASS` 替换为您为 `Identity` 服务中的 placement 用户。注释掉 `[placement]` 部分中的任何其他选项。

##### 完成安装

1. 确定您的计算节点是否支持虚拟机的硬件加速：

   ```bash
   egrep -c '(vmx|svm)' /proc/cpuinfo
   ```

   如果此命令返回值 `1 或更大 `，则您的计算节点支持硬件加速，这通常不需要额外的配置。

   如果此命令返回值 `0`，则您的计算节点不支持硬件加速，您必须将 `libvirt` 配置为使用 QEMU 而不是 KVM。

   - 编辑 `/etc/nova/nova-compute.conf` 文件中的 `[libvirt]` 部分，如下所示：

     ```bash
     [libvirt]
     # ...
     virt_type = qemu
     ```

2. 重新启动 Compute 服务：

   ```bash
   service nova-compute restart
   ```



**将 compute 节点添加到 cell 数据库**

> 在控制节点上运行以下命令

1. 获取管理员凭证以启用仅限管理员的 CLI 命令，然后确认数据库中有计算主机：

   ```bash
   source /etc/keystone/admin-openrc.sh
   
   openstack compute service list --service nova-compute
   ```

2. 发现计算主机：

   ```bash
   su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
   ```

> 添加新的计算节点时，必须在控制器节点上运行 `nova-manage cell_v2 discover_hosts` 以注册这些新计算 节点。或者，您可以在 `/etc/nova/nova.conf` 中：
>
> ```bash
> [scheduler]
> discover_hosts_in_cells_interval = 300
> ```



### Neutrron



#### controler node

安装和配置控制器节点

##### 先决条件

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

     ```bash
     mysql -u root -p000000
     ```

   - 创建 `neutron` 数据库：

     ```bash
     MariaDB [(none)]> CREATE DATABASE neutron;
     ```

   - 授予对 `neutron` 数据库的适当访问权限，将 `NEUTRON_DBPASS` 使用合适的密码：

     ```bash
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
       IDENTIFIED BY '000000';
     ```

   - 退出数据库访问客户端。

2. 获取`管理员`凭证以获取仅限管理员的 CLI 命令的访问权限：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 要创建服务凭证，请完成以下步骤：

   - 创建 `neutron` 用户：

     ```bash
     openstack user create --domain default --password-prompt neutron
     ```

   - 将 `admin` 角色添加到 `neutron` 用户：

     ```bash
     openstack role add --project service --user neutron admin
     ```

   - 创建 `neutron` 服务实体：

     ```bash
     openstack service create --name neutron \
       --description "OpenStack Networking" network
     ```

4. 创建网络服务 API 终端节点：

   ```bash
   openstack endpoint create --region RegionOne \
     network public http://controller:9696
     
   openstack endpoint create --region RegionOne \
     network internal http://controller:9696
   
   openstack endpoint create --region RegionOne \
     network admin http://controller:9696
   ```

   



##### **配置网络选项**

> 本文选择使用`self-service networks`

在*控制器*节点上安装和配置 Networking 组件。

```bash
apt install -y neutron-server neutron-plugin-ml2 \
  neutron-openvswitch-agent neutron-l3-agent neutron-dhcp-agent \
  neutron-metadata-agent
```



**配置服务器组件**

- 编辑 `/etc/neutron/neutron.conf` 文件并完成以下作：

  - 在 `[database]` 部分中，配置数据库访问：

    ```bash
    [database]
    # ...
    connection = mysql+pymysql://neutron:0000000@controller/neutron
    ```

    将 `NEUTRON_DBPASS` 替换为您为数据库选择的密码。

  - 在 `[DEFAULT]` 部分中，启用模块化第 2 层 （ML2） 插件和路由器服务：

    ```bash
    [DEFAULT]
    # ...
    core_plugin = ml2
    service_plugins = router
    ```

  - 在 `[DEFAULT]` 部分中，配置 `RabbitMQ` 消息队列访问：

    ```bash
    [DEFAULT]
    # ...
    transport_url = rabbit://openstack:000000@controller:5672/
    ```

    将 `RABBIT_PASS` 替换为您为 `openstack` 帐户。

  - 在 `[DEFAULT]` 和 `[keystone_authtoken]` 部分中，配置 Identity 服务访问：

    ```bash
    [DEFAULT]
    # ...
    auth_strategy = keystone
    
    [keystone_authtoken]
    # ...
    www_authenticate_uri = http://controller:5000
    auth_url = http://controller:5000
    memcached_servers = controller:11211
    auth_type = password
    project_domain_name = Default
    user_domain_name = Default
    project_name = service
    username = neutron
    password = 000000
    ```

    将 `NEUTRON_PASS` 替换为您为 `neutron` 选择的密码 用户。

  - 在 `[DEFAULT]` 和 `[nova]` 部分中，配置 Networking 以通知 Compute 网络拓扑更改：

    ```bash
    [DEFAULT]
    # ...
    notify_nova_on_port_status_changes = true
    notify_nova_on_port_data_changes = true
    
    [nova]
    # ...
    auth_url = http://controller:5000
    auth_type = password
    project_domain_name = Default
    user_domain_name = Default
    region_name = RegionOne
    project_name = service
    username = nova
    password = 000000
    ```

    将 `NOVA_PASS` 替换为您为 `nova` 选择的密码 用户。

  - 在 `[oslo_concurrency]` 部分中，配置锁定路径：

    ```bash
    [oslo_concurrency]
    # ...
    lock_path = /var/lib/neutron/tmp
    ```

  **配置 Modular Layer 2 （ML2） 插件**

  ML2 插件使用 Linux 桥接机制为实例构建第 2 层（桥接和交换）虚拟网络基础设施。

  - 编辑 `/etc/neutron/plugins/ml2/ml2_conf.ini` 文件并完成以下作：

    - 在 `[ml2]` 部分中，启用FLAT、VLAN 和 VXLAN 网络：

      ```bash
      [ml2]
      # ...
      type_drivers = flat,vlan,vxlan
      ```

    - 在 `[ml2]` 部分中，启用 VXLAN 自助服务网络：

      ```bash
      [ml2]
      # ...
      tenant_network_types = vxlan
      ```

    - 在 `[ml2]` 部分中，启用 Linux 桥和第 2 层填充机制：

      ```bash
      [ml2]
      # ...
      mechanism_drivers = openvswitch,l2population
      ```

      >**警告：**配置 ML2 插件后，删除 `type_drivers` 选项都可能导致数据库不一致。

      > **注意:** Linux 网桥代理仅支持 VXLAN 叠加网络。

    - 在 `[ml2]` 部分中，启用端口安全扩展驱动程序：

      ```bash
      [ml2]
      # ...
      extension_drivers = port_security
      ```

    - 在 `[ml2_type_flat]` 部分中，将提供商虚拟网络配置为平面网络：

      ```bash
      [ml2_type_flat]
      # ...
      flat_networks = provider
      ```

    - 在 `[ml2_type_vxlan]` 部分，配置自助网络的 VXLAN 网络标识符范围：

      ```bash
      [ml2_type_vxlan]
      # ...
      vni_ranges = 1:1000
      ```

##### 配置 **Open vSwitch** 代理

  Linux 桥接代理为实例构建第 2 层（桥接和交换）虚拟网络基础设施并处理安全组。

  - 编辑 `/etc/neutron/plugins/ml2/openvswitch_agent.ini` 文件并完成以下作：

    - 在 `[ovs]` 部分中，将提供商虚拟网络映射到提供商物理网桥，并配置处理叠加网络的物理网络接口的 IP 地址：

      ```bash
      [ovs]
      bridge_mappings = provider:br-provider
      local_ip = 10.0.0.10
      ```
    
    - 确保已创建 `PROVIDER_BRIDGE_NAME` 外部网桥，并且 `PROVIDER_INTERFACE_NAME` 已添加到该桥

      ```bash
      ovs-vsctl add-br br-provider
      ovs-vsctl add-port br-provider ens34
      ```
    
    - 在 `[agent]` 部分中，启用 VXLAN 叠加网络并启用第 2 层填充：

      ```bash
      [agent]
      tunnel_types = vxlan
      l2_population = true
      ```
    
    - 在 `[securitygroup]` 部分中，启用安全组并配置 Open vSwitch 本机或混合 iptables 防火墙驱动程序：

      ```bash
      [securitygroup]
      # ...
      enable_security_group = true
      firewall_driver = openvswitch
      #firewall_driver = iptables_hybrid
      ```
    
    - 如果使用混合 iptables 防火墙驱动程序，请验证以下所有 `sysctl` 值是否都设置为 `1`，确保您的 Linux 作系统内核支持网桥过滤器：

      ```bash
      net.bridge.bridge-nf-call-iptables
      net.bridge.bridge-nf-call-ip6tables
      ```
    
      要启用网络桥支持，通常需要加载 `br_netfilter` 内核模块。
    
  - 编辑网卡配置文件`/etc/netplan/50-cloud-init.yaml`，使得外网网卡`ens34`绑定创建的`br-provider`网桥配置持久化
  
    ```bash
    network:
      version: 2
      renderer: networkd
      ethernets:
        ens33:
          addresses: [10.0.0.10/24]
          nameservers:
            addresses: [8.8.8.8]  # 补充 DNS
        ens34:
          dhcp4: no
          dhcp6: no
      bridges:
        br-provider:
          openvswitch: {}
          interfaces: [ens34]
          addresses: [192.168.35.10/24]
          nameservers:
            addresses: [8.8.8.8]
          routes:
          - to: default
            via: 192.168.35.2  # 仅保留一个默认网关
    ```
    
    修改文件权限及应用配置文件
    
    ```bash
    chmod 600 /etc/netplan/01-netcfg.yaml
    netplan apply
    ```
    
    

配置 **layer-3 Agent**

第 3 层 （L3） 代理为自助式虚拟网络提供路由和 NAT 服务。

- 编辑 `/etc/neutron/l3_agent.ini` 文件并完成以下作: 

  - 在 `[DEFAULT]` 部分中，配置 Open vSwitch 接口驱动程序：

    ```bash
    [DEFAULT]
    # ...
    interface_driver = openvswitch
    ```

配置 **DHCP** 代理

DHCP 代理为虚拟网络提供 DHCP 服务。

- 编辑 `/etc/neutron/dhcp_agent.ini` 文件并完成以下作：

  - 在 `[DEFAULT]` 部分中，配置 Open vSwitch 接口驱动程序、Dnsmasq DHCP 驱动程序并启用隔离元数据，以便提供商网络上的实例可以通过网络访问元数据：

    ```bash
    [DEFAULT]
    # ...
    interface_driver = openvswitch
    dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
    enable_isolated_metadata = true
    ```



##### **配置元数据代理**

元数据代理提供配置信息，例如实例的凭据。

- 编辑 `/etc/neutron/metadata_agent.ini` 文件并完成以下作：

  - 在 `[DEFAULT]` 部分中，配置元数据主机和共享密钥：

    ```bash
    [DEFAULT]
    # ...
    nova_metadata_host = controller
    metadata_proxy_shared_secret = 000000
    ```

    将 `METADATA_SECRET` 替换为元数据代理的合适密钥

    

##### **配置 Compute 服务以使用 Networking 服务**

- 编辑 `/etc/nova/nova.conf` 文件并执行以下作：

  - 在 `[neutron]` 部分，配置访问参数，启用元数据代理，并配置 secret：

    ```bash
    [neutron]
    # ...
    auth_url = http://controller:5000
    auth_type = password
    project_domain_name = Default
    user_domain_name = Default
    region_name = RegionOne
    project_name = service
    username = neutron
    password = 000000
    service_metadata_proxy = true
    metadata_proxy_shared_secret = 000000
    ```

    将 `NEUTRON_PASS` 替换为您为 `neutron` 选择的密码 用户。
    将 `METADATA_SECRET` 替换为您为元数据代理选择的密钥。



##### **完成安装**

1. 填充数据库：

   ```bash
   su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf \
     --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
   ```

2. 重新启动 Compute API 服务：

   ```bash
   service nova-api restart
   ```

3. 重新启动 Networking 服务。

   > 由于选择的是`self-service network`,所以要执行以下重启选项

   ```bash
   service neutron-server restart
   service neutron-openvswitch-agent restart
   service neutron-dhcp-agent restart
   service neutron-metadata-agent restart
   
   # 3层代理只有`self-service network`网络选项需要
   service neutron-l3-agent restart
   ```

   

#### compute node



安装和配置计算节点

计算节点处理实例的连接和安全组。



##### 安装组件

```bash
apt install -y neutron-openvswitch-agent
```

##### **配置通用组件**

网络通用组件配置包括鉴权机制、消息队列和插件。

- 编辑 `/etc/neutron/neutron.conf` 文件并完成以下作：

  - 在 `[database]` 部分中，注释掉任何`连接`选项，因为计算节点不直接访问数据库。

    ```bash
    [database]
    #connection = sqlite:////var/lib/neutron/neutron.sqlite
    ```

  - 在 `[DEFAULT]` 部分中，配置 `RabbitMQ` 消息队列访问：

    ```bash
    [DEFAULT]
    # ...
    transport_url = rabbit://openstack:000000@controller
    ```

    将 `RABBIT_PASS` 替换为您为 `openstack` 选择的密码 account 的 RabbitMQ 中。

- 在 `[oslo_concurrency]` 部分中，配置锁定路径：

  ```bash
  [oslo_concurrency]
  # ...
  lock_path = /var/lib/neutron/tmp
  ```



##### **配置网络选项**

> 本文选择使用`self-service networks`

在*计算*节点上配置 Networking 组件。



##### 配置 **Open vSwitch** 代理

Open vSwitch 代理为实例构建第 2 层（桥接和交换）虚拟网络基础设施并处理安全组。

- 编辑 `/etc/neutron/plugins/ml2/openvswitch_agent.ini` 文件并完成以下作：

  - 在 `[ovs]` 部分中，将提供商虚拟网络映射到提供商物理网桥，并配置处理叠加网络的物理网络接口的 IP 地址：

    > **注意:**这里的openvswitch需要创建网桥，并写入配置文件
    >
    > PROVIDER_BRIDGE_NAME ： 创建的网桥名称
    >
    > PROVIDER_INTERFACE_NAME： 外部网卡设备名称
  
    ```bash
    [ovs]
    bridge_mappings = provider:br-provider
    # 不同的计算节点填写自己的管理网段IP
    local_ip = 10.0.0.20
    ```
  
  - **确保创建PROVIDER_BRIDGE_NAME外部桥接，并将PROVIDER_INTERFACE_NAME添加到该桥接**
  
    ```bash
    ovs-vsctl add-br br-provider
    ovs-vsctl add-port br-provider ens34
    ```
  
  - 在 `[agent]` 部分中，启用 VXLAN 叠加网络并启用第 2 层填充：

    ```bash
    [agent]
    tunnel_types = vxlan
    l2_population = true
    ```
  
  - 在 `[securitygroup]` 部分中，启用安全组并配置 Open vSwitch 本机或混合 iptables 防火墙驱动程序：
  
    ```bash
    [securitygroup]
    # ...
    enable_security_group = true
    firewall_driver = openvswitch
    #firewall_driver = iptables_hybrid
    ```
  
  - 如果使用混合 iptables 防火墙驱动程序，请验证以下所有 `sysctl` 值是否都设置为 `1`，确保您的 Linux 作系统内核支持网桥过滤器：
  
    ```bash
    net.bridge.bridge-nf-call-iptables
    net.bridge.bridge-nf-call-ip6tables
    ```
  
    要启用网络桥支持，通常需要加载 `br_netfilter` 内核模块
  
  
  
  ###### 配置网卡文件启用ovs网桥
  
  编辑网卡配置文件`/etc/netplan/50-cloud-init.yaml`，使得外网网卡`ens34`绑定创建的`br-provider`网桥配置持久化
  
  ```bash
  network:
    version: 2
    renderer: networkd
    ethernets:
      ens33:
        addresses: [10.0.0.20/24]
        nameservers:
          addresses: [8.8.8.8]  # 补充 DNS
      ens34:
        dhcp4: no
        dhcp6: no
    bridges:
      br-provider:
        openvswitch: {}
        interfaces: [ens34]
        addresses: [192.168.35.20/24]
        nameservers:
          addresses: [8.8.8.8]
        routes:
        - to: default
          via: 192.168.35.2  # 仅保留一个默认网关
  ```
  
  修改文件权限及应用配置文件
  
  ```bash
  chmod 600 /etc/netplan/01-netcfg.yaml
  netplan apply
  ```



##### **配置 Compute 服务以使用 Networking 服务**

- 编辑 `/etc/nova/nova.conf` 文件并完成以下作：

  - 在 `[neutron]` 部分中，配置访问参数：

    ```bash
    [neutron]
    # ...
    auth_url = http://controller:5000
    auth_type = password
    project_domain_name = Default
    user_domain_name = Default
    region_name = RegionOne
    project_name = service
    username = neutron
    password = 000000
    ```

    **将NEUTRON_PASS替换为您在身份服务中为neutron用户选择的密码。**



##### **完成安装**

1. 重新启动 Compute 服务：

   ```bash
   service neutron-openvswitch-agent restart
   ```

   

2. 重新启动 Linux 桥接代理：

   ```bash
   service neutron-openvswitch-agent restart
   ```




创建网络

```bash
openstack network create  --share --external \
  --provider-physical-network provider \
  --provider-network-type flat provider
  
  openstack subnet create --network provider \
  --allocation-pool start=192.168.35.100,end=192.168.35.200 \
  --dns-nameserver 8.8.8.8 --gateway 192.168.35.2 \
  --subnet-range 192.168.35.0/24 provider
```



### Dashboard



#### 安装和配置组件

1. 安装软件包：

   ```bash
   apt install openstack-dashboard -y
   ```

2. 编辑 `/etc/openstack-dashboard/local_settings.py` 文件并完成以下作：

   - 将仪表板配置为使用 OpenStack 服务 `controller` 节点：

     ```bash
     OPENSTACK_HOST = "controller"
     ```

   - 在 Dashboard configuration （控制面板配置） 部分中，允许您的主机访问 Dashboard：

     ```bash
     ALLOWED_HOSTS = ['*',]
     ```

   - 配置 `memcached` 会话存储服务： **官方提供的选项，测试中发现不需要更改PyMemcacheCache为MemcachedCache**

     ```bash
     # 可以更改
     SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
     
     # 不需要按官方更改
     CACHES = {
         'default': {
              'BACKEND': 'django.core.cache.backends.memcached.PyMemcacheCache',
              'LOCATION': 'controller:11211',
         }
     }
     ```

   - 启用 Identity API 版本 3：**测试后发现，该地方有错，会导致进入dashboard内部的认证失败的问题**

     ```bash
     OPENSTACK_KEYSTONE_URL = "http://%s/identity/v3" % OPENSTACK_HOST
     
     # 改成
     OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST
     ```

   - 启用对域的支持：

     ```bash
     OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
     ```

   - Configure API versions: 配置 API 版本：

     ```bash
     OPENSTACK_API_VERSIONS = {
         "identity": 3,
         "image": 2,
         "volume": 3,
     }
     ```

   - 将 `Default` 配置为通过功能板创建的用户的默认域：

     ```bash
     OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"
     ```

   - 将 `user` 配置为通过功能板创建的用户的默认角色：

     ```bash
     OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"
     ```

   - 如果您选择了联网选项 1，请禁用对第 3 层联网服务的支持：**由于前面我们选择的是网络选项2，可以忽略这一步**

     ```bash
     OPENSTACK_NEUTRON_NETWORK = {
         ...
         'enable_router': False,
         'enable_quotas': False,
         'enable_ipv6': False,
         'enable_distributed_router': False,
         'enable_ha_router': False,
         'enable_fip_topology_check': False,
     }
     ```

   - 配置时区：

     ```bash
     TIME_ZONE = "Asia/Shanghai"
     ```

3. 将以下行添加到 `/etc/apache2/conf-available/openstack-dashboard.conf` 如果不包括在内。

   ```bash
   WSGIApplicationGroup %{GLOBAL}
   ```



#### **完成安装**

- 重新加载 Web 服务器配置：

  ```bash
  systemctl reload apache2.service
  ```




#### 报错解决方案

按照官网方案尝试之后发现，dashboard界面总是无法正常使用

![](./../../md_images/openstack/horzion-error.png)

在`/etc/openstack-dashboard/local-settings.py`修改`DEBUG`模式为`True`后，通过调试模式看到，系统读取到无法识别到模块`PyMemcached`

1. 更新`python`的模块

   - 由于`ubuntu-22.04`默认没有`pip`工具，所以先安装一个`python3-pip`

     ```bash
     apt-get install -y python3-pip
     ```

   - 更新两个`python`模块

     ```bash
     python3 -m pip install --upgrade packaging
     ...
     # Successfully installed packaging-25.0
     
     python3 -m pip install --upgrade django-debreach
     ...
     # Successfully installed django-debreach-2.1.0
     ```

2. 使用`/usr/share/openstack-dashboard/`目录下的python脚本`manage.py`对horzion的静态页面文件进行收集与压缩

   - 使用`python3 manage.py compress`命令

     ```bash
     root@controller:/usr/share/openstack-dashboard# python3 manage.py compress
     /usr/lib/python3/dist-packages/django/conf/__init__.py:267: RemovedInDjango50Warning: The USE_L10N setting is deprecated. Starting with Django 5.0, localized formatting of data will always be enabled. For example Django will display numbers and dates using the format of the current locale.
       warnings.warn(USE_L10N_DEPRECATED_MSG, RemovedInDjango50Warning)
     /usr/local/lib/python3.10/dist-packages/debreach/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
       version_info = version.StrictVersion(__version__).version
     Compressing... done
     Compressed 8 block(s) from 18 template(s) for 3 context(s).
     ```

   - 后续如果修改了静态资源，可以强制重新压缩：

     ```bash
     python3 manage.py compress --force
     ```

     **注意点： ** 确保`settings.py`中启用了压缩和离线模式：

     ```bash
     COMPRESS_ENABLED = True
     COMPRESS_OFFLINE = True
     ```

![image-20250517162754981](D:/cloud_study/md_images/openstack/horzion-successful.png)



### Cinder



#### controller node



##### **先决条件**

在安装和配置 Block Storage 服务之前，您需要 必须创建数据库、服务凭证和 API 终端节点。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端连接数据库 server 作为用户：`root`

     ```bash
     mysql -uroot -p000000
     ```

   - 创建数据库：`cinder`

     ```bash
     MariaDB [(none)]> CREATE DATABASE cinder;
     ```

   - 授予对数据库的适当访问权限：`cinder`

     ```bash
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' \
       IDENTIFIED BY '000000';
     MariaDB [(none)]> GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' \
       IDENTIFIED BY '000000';
     ```

     替换为合适的密码。`CINDER_DBPASS`

   - 退出数据库访问客户端。

2. 获取凭据以获取仅限管理员的访问权限 CLI 命令：`admin`

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 要创建服务凭证，请完成以下步骤：

   - 创建用户`cinder`：

     ```bash
     openstack user create --domain default --password-prompt cinder
     
     // 输入密码
     User Password:
     Repeat User Password:
     ```

   - 将角色`admin`添加到用户`cinder`：

     ```bash
     openstack role add --project service --user cinder admin
     ```

   - 创建服务实体：`cinderv3`

     ```bash
     openstack service create --name cinderv3 \
       --description "OpenStack Block Storage" volumev3
     ```

4. 创建 Block Storage 服务 API 端点：

   ```bash
   openstack endpoint create --region RegionOne \
     volumev3 public http://controller:8776/v3/%\(project_id\)s
     
   openstack endpoint create --region RegionOne \
     volumev3 internal http://controller:8776/v3/%\(project_id\)s
     
   openstack endpoint create --region RegionOne \
     volumev3 admin http://controller:8776/v3/%\(project_id\)s
   ```

   

##### **安装和配置组件**

1. 安装软件包：

   ```bash
   apt install cinder-api cinder-scheduler -y
   ```

2. 编辑文件并完成 作：`/etc/cinder/cinder.conf`

   - 在该部分`[database]`中，配置数据库访问：

     ```bash
     [database]
     # ...
     connection = mysql+pymysql://cinder:000000@controller/cinder
     ```

     替换为您为 Block Storage 数据库。`CINDER_DBPASS`

   - 在该部分`[DEFAULT]`中，配置消息队列访问：`RabbitMQ`

     ```bash
     [DEFAULT]
     # ...
     transport_url = rabbit://openstack:000000@controller
     ```

     替换为您在`RabbitMQ`中为帐户`openstack`选择的密码`RABBIT_PASS`。

   - 在`[DEFAULT]` 和 `[keystone_authtoken]`部分中， 配置 Identity Service 访问：

     ```bash
     [DEFAULT]
     # ...
     auth_strategy = keystone
     
     [keystone_authtoken]
     # ...
     www_authenticate_uri = http://controller:5000
     auth_url = http://controller:5000
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = default
     user_domain_name = default
     project_name = service
     username = cinder
     password = 000000
     ```

     替换为您为 Identity 服务中的用户`cinder`的密码。`CINDER_PASS`

   - 在该部分中，将选项配置为 使用控制器节点的管理接口 IP 地址：`[DEFAULT]``my_ip`

     ```bash
     [DEFAULT]
     # ...
     my_ip = 10.0.0.10
     ```

3. 在该部分中，配置锁定路径：`[oslo_concurrency]`

   ```bash
   [oslo_concurrency]
   # ...
   lock_path = /var/lib/cinder/tmp
   ```

4. 填充 Block Storage 数据库：

   ```bash
   su -s /bin/sh -c "cinder-manage db sync" cinder
   ```

>注意: 忽略此输出中的任何弃用消息。

##### **配置 Compute 以使用 Block Storage**

1. 编辑文件并添加以下内容 到它：`/etc/nova/nova.conf`

   ```bash
   [cinder]
   os_region_name = RegionOne
   ```

   

**完成安装**

1. 重新启动 Compute API 服务：

   ```bash
   service nova-api restart
   ```

2. 重新启动 Block Storage 服务：

   ```bash
   service cinder-scheduler restart
   service apache2 restart
   ```

   

#### storage node



##### 先决条件

在 storage node 中，您必须准备存储设备。

1. 安装支持的实用程序包：

   ```bash
   apt install lvm2 thin-provisioning-tools -y
   ```

2. 创建 LVM 物理卷 ：`/dev/sdb`

   ```bash
   # pvcreate /dev/sdb
   
   Physical volume "/dev/sdb" successfully created
   ```

3. 创建 LVM 卷组 ：`cinder-volumes`

   ```bash
   # vgcreate cinder-volumes /dev/sdb
   
   Volume group "cinder-volumes" successfully created
   ```

   Block Storage 服务在此卷组中创建逻辑卷。

4. 只有实例才能访问块存储卷。但是， 底层作系统管理与 卷。默认情况下，LVM 卷扫描工具会扫描目录中的块存储设备 包含卷。如果项目在其卷上使用 LVM，则扫描 工具检测这些卷并尝试缓存它们，这可能会导致 底层作系统存在各种问题 和项目体积。您必须重新配置 LVM 以仅扫描设备 ，其中包含卷组。编辑文件并完成以下作：`/dev` `cinder-volumes` `/etc/lvm/lvm.conf`

   - 在该部分中，添加接受设备并拒绝所有其他设备的过滤器：`devices``/dev/sdb`

     ```bash
     devices {
     ...
     filter = [ "a/sdb/", "r/.*/"]
     ```

     filter 数组中的每个项目都以 for **accept** 或 for **reject** 开头，并包含一个正则表达式 设备名称。数组必须以 结尾才能拒绝任何 剩余设备。您可以使用 **vgs -vvvv** 命令 以测试过滤器。`a``r``r/.*/`

     > 警告
     >
     > 如果您的存储节点在作系统磁盘上使用 LVM，则 还必须将关联的设备添加到过滤器中。例如 如果设备包含作系统：`/dev/sda`
     >
     > ```
     > filter = [ "a/sda/", "a/sdb/", "r/.*/"]
     > ```
     >
     > 同样，如果您的计算节点在运行 system disk 时，还必须修改这些节点上文件中的 filter 以仅包含 作系统磁盘。例如，如果设备包含作系统：`/etc/lvm/lvm.conf``/dev/sda`
     >
     > ```
     > filter = [ "a/sda/", "r/.*/"]
     > ```

##### 安装和配置组件

1. 安装软件包：

   ```bash
   apt install cinder-volume tgt -y
   ```

2. 编辑文件 并完成以下作：`/etc/cinder/cinder.conf`

   - 在该部分中，配置数据库访问：`[database]`

     ```bash
     [database]
     # ...
     connection = mysql+pymysql://cinder:000000@controller/cinder
     ```

     替换为您为 Block Storage 数据库。`CINDER_DBPASS`

   - 在该部分中，配置消息队列访问：`[DEFAULT]``RabbitMQ`

     ```bash
     [DEFAULT]
     # ...
     transport_url = rabbit://openstack:000000@controller
     ```

     替换为您为`RabbitMQ`中的帐户`openstack`。`RABBIT_PASS`

   - 在 和 部分中， 配置 Identity Service 访问：`[DEFAULT]``[keystone_authtoken]`

     ```bash
     [DEFAULT]
     # ...
     auth_strategy = keystone
     
     [keystone_authtoken]
     # ...
     www_authenticate_uri = http://controller:5000
     auth_url = http://controller:5000
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = default
     user_domain_name = default
     project_name = service
     username = cinder
     password = 000000
     ```

     替换为您在 Identity 服务中为用户`cinder`选择的密码`CINDER_PASS`。

     > 注意: 注释掉或删除该部分中的任何其他选项。`[keystone_authtoken]`

   - 在该部分中，配置选项：`[DEFAULT]``my_ip`

       ```bash
       [DEFAULT]
       # ...
       my_ip = MANAGEMENT_INTERFACE_IP_ADDRESS
       ```

       替换为 IP 地址 `MANAGEMENT_INTERFACE_IP_ADDRESS`存储节点上的管理网络接口， 通常为 10.0.0.41 表示[示例架构](https://docs.openstack.org/install-guide/overview.html#example-architecture)中的第一个节点。

       如果使用的是计算节点安装cinder部分，使用计算节点的管理网络接口10.0.0.20 / 10.0.0.30

   - 在该部分中，使用 LVM 驱动程序、卷组、iSCSI 协议、 和适当的 iSCSI 服务：`[lvm]``cinder-volumes`

       ```bash
       [lvm]
       # ...
       volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
       volume_group = cinder-volumes
       target_protocol = iscsi
       target_helper = tgtadm
       ```

   - 在该部分中，启用 LVM 后端：`[DEFAULT]`

       ```bash
       [DEFAULT]
       # ...
       enabled_backends = lvm
       ```

       > 注意: 后端名称是任意的。例如，本指南 使用驱动程序的名称作为后端的名称。

   - 在该部分中，配置 图片服务 API：`[DEFAULT]`

       ```bash
       [DEFAULT]
       # ...
       glance_api_servers = http://controller:9292
       ```

   - 在该部分中，配置锁定路径：`[oslo_concurrency]`

       ```bash
       [oslo_concurrency]
       # ...
       lock_path = /var/lib/cinder/tmp
       ```

3. 创建文件 替换为以下数据：`/etc/tgt/conf.d/cinder.conf`

   > s注意:仅在使用 tgt target 时执行此步骤。

   ```bash
   include /var/lib/cinder/volumes/*
   ```

**完成安装**

1. 重新启动 Block Storage 卷服务，包括其依赖项：

   ```bash
   service tgt restart
   service cinder-volume restart
   ```





#### backup service



（可选）安装和配置备份服务。为简单起见， 此配置使用 Block Storage 节点和 Object Storage （swift） 驱动程序，因此取决于 [Object Storage 服务](https://docs.openstack.org/swift/latest/install/)。

> 注意:必须先[安装和配置存储节点](https://docs.openstack.org/cinder/2024.1/configuration/block-storage/config-options.html#cinder-storage) 以安装和配置备份服务。

##### **安装和配置组件**

>  注意:在 Block Storage 节点上执行这些步骤。

1. 安装软件包：

   ```bash
   apt install cinder-backup -y
   ```

2. 编辑文件 并完成以下作：`/etc/cinder/cinder.conf`

   - 在该部分中，配置备份选项：`[DEFAULT]`

     ```bash
     [DEFAULT]
     # ...
     backup_driver = cinder.backup.drivers.swift.SwiftBackupDriver
     backup_swift_url = SWIFT_URL
     ```

     替换为 Object Storage 服务的 URL。这 URL 可以通过显示对象存储 API 端点来找到：`SWIFT_URL`



##### **完成安装**

重新启动 Block Storage 备份服务：

```bash
service cinder-backup restart
```

验证 Cinder

验证 Block Storage 服务的运行情况。

```bash
openstack volume service list
```



### Swift



#### controller node

##### 先决条件

代理服务依赖于身份验证和授权机制，例如 作为 Identity 服务。但是，与其他服务不同的是，它还提供了一个 内部机制，允许它在没有任何其他 OpenStack 的情况下运行 服务业。在配置 Object Storage 服务之前，您必须 创建服务凭证和 API 终端节点。

>   注意: Object Storage 服务不使用控制器上的 SQL 数据库 节点。相反，它在每个存储节点上使用分布式 SQLite 数据库。

1. 获取凭证以获取仅限管理员的 CLI 命令的访问权限：`admin`

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

2. 要创建 Identity 服务凭证，请完成以下步骤：

   - 创建用户：`swift`

     ```bash
     openstack user create --domain default --password-prompt swift
     User Password:
     Repeat User Password:
     ```

   - 将角色`admin`添加到用户`swift`：

     ```bash
     openstack role add --project service --user swift admin
     ```

   - 创建服务实体：`swift`

     ```bash
     openstack service create --name swift \
       --description "OpenStack Object Storage" object-store
     ```

3. 创建 Object Storage 服务 API 端点：

   ```bash
   openstack endpoint create --region RegionOne \
     object-store public http://controller:8080/v1/AUTH_%\(project_id\)s
   
   openstack endpoint create --region RegionOne \
     object-store internal http://controller:8080/v1/AUTH_%\(project_id\)s
     
   openstack endpoint create --region RegionOne \
     object-store admin http://controller:8080/v1
   ```

   

##### **安装和配置组件**

1. 安装软件包：

   ```bash
   apt-get install swift swift-proxy python3-swiftclient \
     python3-keystoneclient python3-keystonemiddleware \
     memcached -y
   ```

   1. 创建目录。`/etc/swift`

     ```bash
     mkdir /etc/swift
     ```

   2. 从 Object Storage 获取代理服务配置文件 源存储库：

      ```bash
      # 使用 Swift 自带的tempauth模块，独立认证,
      curl -o /etc/swift/proxy-server.conf https://opendev.org/openstack/swift/raw/branch/master/etc/proxy-server.conf-sample
      ```
   
   3.  编辑文件并完成 作：`/etc/swift/proxy-server.conf`
   
       - 在该部分中，配置 bind port、user 和 配置目录：`[DEFAULT]`
   
         ```bash
         [DEFAULT]
         ...
         bind_port = 8080
         user = swift
         swift_dir = /etc/swift
         ```
   
       - 在该部分`[pipeline:main]`中，删除 and modules 并添加 和 modules：`tempurl``tempauth``authtoken``keystoneauth`
   
         ```bash
         # 依赖 OpenStack Keystone 进行统一认证。
         [pipeline:main]
         pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server
         ```
   
         >   注意: 请勿更改模块的顺序。
   
       - 在该部分中，启用自动帐户创建：`[app:proxy-server]`
   
         ```bash
         [app:proxy-server]
         use = egg:swift#proxy
         ...
         account_autocreate = True
         ```
   
       - 在该部分中，配置作员角色：`[filter:keystoneauth]`
   
         ```bash
         [filter:keystoneauth]
         use = egg:swift#keystoneauth
         ...
         operator_roles = admin,user
         ```
   
       - 在该部分中，配置 Identity Service 访问：`[filter:authtoken]`
   
         ```bash
         [filter:authtoken]
         paste.filter_factory = keystonemiddleware.auth_token:filter_factory
         www_authenticate_uri = http://controller:5000
         auth_url = http://controller:5000
         memcached_servers = controller:11211
         auth_type = password
         project_domain_id = default
         user_domain_id = default
         project_name = service
         username = swift
         password = 000000
         delay_auth_decision = True
         ```
         
         替换为您在 Identity 服务中为用户`swift`选择的密码`SWIFT_PASS` 
         
       - 在该`[filter:cache]`部分中，配置位置：`memcached`
       
         ```bash
         [filter:cache]
         use = egg:swift#memcache
         ...
         memcache_servers = controller:11211
         ```



#### storage nodes



##### 先决条件

在 storage nodes 中，您必须准备存储设备。

> 注意: 在每个存储节点上执行这些步骤。

1. 安装支持的实用程序包：

   ```bash
   apt-get install xfsprogs rsync -
   ```

2. 将 `/dev/sdb`和`/dev/sdc` 设备格式化为 XFS：

   ```bash
   mkfs.xfs /dev/sdb
   mkfs.xfs /dev/sdc
   ```

3. 创建挂载点目录结构：

   ```bash
   mkdir -p /srv/node/sdb
   mkdir -p /srv/node/sdc
   ```

4. 找到新分区的 UUID：

   ```bash
   blkid
   ```

5. 编辑文件并向其添加以下内容：`/etc/fstab`

   ```bash
   UUID="<UUID-from-output-above>" /srv/node/sdb xfs noatime 0 2
   UUID="<UUID-from-output-above>" /srv/node/sdc xfs noatime 0 2
   ```

6. 挂载设备：

   ```bash
   mount /srv/node/sdb
   mount /srv/node/sdc
   ```

7. 创建或编辑文件以包含以下内容：`/etc/rsyncd.conf`

   ```bash
   uid = swift
   gid = swift
   log file = /var/log/rsyncd.log
   pid file = /var/run/rsyncd.pid
   address = 10.0.0.20
   
   [account]
   max connections = 2
   path = /srv/node/
   read only = False
   lock file = /var/lock/account.lock
   
   [container]
   max connections = 2
   path = /srv/node/
   read only = False
   lock file = /var/lock/container.lock
   
   [object]
   max connections = 2
   path = /srv/node/
   read only = False
   lock file = /var/lock/object.lock
   ```

   替换为 存储节点上的管理网络。`MANAGEMENT_INTERFACE_IP_ADDRESS`

   

8. 编辑文件并启用服务：`/etc/default/rsync``rsync`

   ```bash
   RSYNC_ENABLE=true
   ```

9. 启动服务：`rsync`

   ```bash
   service rsync start
   ```



##### 安装和配置组件

1. 安装软件包：

   ```bash
   apt-get install swift swift-account swift-container swift-object -y
   ```

2. 获取 Accounting、Container 和 Object Service 配置 Object Storage 源存储库中的文件：

   ```bash
   curl -o /etc/swift/account-server.conf https://opendev.org/openstack/swift/raw/branch/master/etc/account-server.conf-sample
   curl -o /etc/swift/container-server.conf https://opendev.org/openstack/swift/raw/branch/master/etc/container-server.conf-sample
   curl -o /etc/swift/object-server.conf https://opendev.org/openstack/swift/raw/branch/master/etc/object-server.conf-sample
   ```

3. 编辑文件并完成 作：`/etc/swift/account-server.conf`

   - 在该部分中，配置绑定 IP 地址、绑定端口、 user、configuration directory 和 mount point directory：`[DEFAULT]`

     ```bash
     [DEFAULT]
     ...
     bind_ip = 10.0.0.20
     bind_port = 6202
     user = swift
     swift_dir = /etc/swift
     devices = /srv/node
     mount_check = True
     ```

     替换为 存储节点上的管理网络。`MANAGEMENT_INTERFACE_IP_ADDRESS`

   - 在该部分中，启用相应的模块：`[pipeline:main]`

     ```bash
     [pipeline:main]
     pipeline = healthcheck recon account-server
     ```

     **backend_ratelimit模块**

     curl中的示例文件包含该模块，作用是对后端存储操作（如对象读写）的请求速率进行限制，防止某些账户或容器的请求占用过多带宽或 IOPS

     **典型参数** （需在 `[filter:backend_ratelimit]` 中配置）：

     ```bash
     [filter:backend_ratelimit]
     paste.filter_factory = swift.common.middleware.backend_ratelimit:filter_factory
     rate_limit_per_sec = 100  # 每秒允许的请求数
     burst_length = 200        # 突发请求允许的最大请求数
     ```

     **触发条件** ：
     当请求超过设定的速率阈值时，返回 `HTTP 498 (Request Header Fields Too Large)` 错误

4. 编辑文件并完成 作：`/etc/swift/container-server.conf`

   - 在该部分中，配置绑定 IP 地址、绑定端口、 user、configuration directory 和 mount point directory：`[DEFAULT]`

     ```bash
     [DEFAULT]
     ...
     bind_ip = 10.0.0.20
     bind_port = 6201
     user = swift
     swift_dir = /etc/swift
     devices = /srv/node
     mount_check = True
     ```

     替换为 存储节点上的管理网络。`MANAGEMENT_INTERFACE_IP_ADDRESS`

   - 在该部分中，启用相应的模块：`[pipeline:main]`

     ```bash
     [pipeline:main]
     pipeline = healthcheck recon object-server
     ```

   - 在该部分中，配置 recon （meters） 缓存 并锁定目录：`[filter:recon]`

     ```bash
     [filter:recon]
     use = egg:swift#recon
     ...
     recon_cache_path = /var/cache/swift
     recon_lock_path = /var/lock
     ```

5. 编辑文件并完成 作：`/etc/swift/object-server.conf`

   - 在该部分中，配置绑定 IP 地址、绑定端口、 user、configuration directory 和 mount point directory：`[DEFAULT]`

     ```bash
     [DEFAULT]
     ...
     bind_ip = 10.0.0.20
     bind_port = 6200
     user = swift
     swift_dir = /etc/swift
     devices = /srv/node
     mount_check = True
     ```

     替换为 存储节点上的管理网络。`MANAGEMENT_INTERFACE_IP_ADDRESS`

   - 在该部分中，启用相应的模块：`[pipeline:main]`

     ```bash
     [pipeline:main]
     pipeline = healthcheck recon object-server
     ```

   - 在该部分中，配置 recon （meters） 缓存 并锁定目录：`[filter:recon]`

     ```bash
     [filter:recon]
     use = egg:swift#recon
     ...
     recon_cache_path = /var/cache/swift
     recon_lock_path = /var/lock
     ```

6. 确保挂载点目录结构的正确所有权：

   ```bash
   chown -R swift:swift /srv/node
   ```

7. 创建目录并确保其所有权正确：`recon`

   ```bash
   mkdir -p /var/cache/swift
   chown -R root:swift /var/cache/swift
   chmod -R 775 /var/cache/swift
   ```



#### 创建和分发初始环

> **在控制节点上执行这些步骤**

##### 创建帐户环

账户服务器使用账户环来维护容器列表。

1. 切换到 `/etc/swift` 目录。

2. 创建基本 `container.builder` 文件：

   ```bash
   swift-ring-builder account.builder create 10 2 24
   ```

3. 将每个==存储节点==添加到环中：

   ```bash
   swift-ring-builder account.builder \
     add --region 1 --zone 1 --ip 10.0.0.20 --port 6202 \
     --device sdb --weight DEVICE_WEIGHT
   ```

   替换为 `STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS` 存储节点上管理网络的 IP 地址。将 `DEVICE_NAME` 替换为同一存储节点上的存储设备名称。例如，使用 Install 中的第一个存储节点，并使用 `/dev/sdc` [配置存储节点](https://docs.openstack.org/swift/2024.1/install/storage-install.html#storage) 存储设备和重量 100：

   ```bash
   root@controller:/etc/swift# swift-ring-builder account.builder \
     add --region 1 --zone 1 --ip 10.0.0.20 --port 6202 \
     --device sdc --weight 100
   Device d0r1z1-10.0.0.20:6202R10.0.0.20:6202/sdc_"" with 100.0 weight got id 0
   root@controller:/etc/swift# swift-ring-builder account.builder \
     add --region 1 --zone 1 --ip 10.0.0.30 --port 6202 \
     --device sdc --weight 100
   Device d1r1z1-10.0.0.30:6202R10.0.0.30:6202/sdc_"" with 100.0 weight got id 1
   ```

   对每个存储节点上的每个存储设备重复此命令。在 示例架构中，使用该命令的四种变体：

   	如果只有sdc，只需要执行一块设备的命令

   ```bash
   swift-ring-builder account.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6202 --device sdb --weight 100
   
   swift-ring-builder account.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6202 --device sdc --weight 100
   
   swift-ring-builder account.builder add \
     --region 1 --zone 2 --ip 10.0.0.30 --port 6202 --device sdb --weight 100
   
   swift-ring-builder account.builder add \
     --region 1 --zone 2 --ip 10.0.0.30 --port 6202 --device sdc --weight 100
   ```

4. 验证环内容：

   ```bash
   root@controller:/etc/swift# swift-ring-builder account.builder
   account.builder, build version 2, id 3233f36bd4b240be9817cfd50b01bd83
   1024 partitions, 3.000000 replicas, 1 regions, 1 zones, 2 devices, 100.00 balance, 0.00 dispersion
   The minimum number of hours before a partition can be reassigned is 1 (0:00:00 remaining)
   The overload factor is 0.00% (0.000000)
   Ring file account.ring.gz not found, probably it hasn't been written yet
   Devices:   id region zone ip address:port replication ip:port  name weight partitions balance flags meta
               0      1    1  10.0.0.20:6202      10.0.0.20:6202   sdc 100.00          0 -100.00
               1      1    1  10.0.0.30:6202      10.0.0.30:6202   sdc 100.00          0 -100.00
   ```

5. 重新平衡环：

   ```bash
   swift-ring-builder account.builder rebalance
   Reassigned 1024 (100.00%) partitions. Balance is now 0.00.  Dispersion is now 0.00
   ```



##### 创建容器环

容器服务器使用容器环来维护对象列表。但是，它不会追踪对象位置。

1. 切换到 `/etc/swift` 目录。

2. 创建基本 `container.builder` 文件：

   ```bash
   swift-ring-builder container.builder create 10 2 24
   ```

   

3. 将每个存储节点添加到环中：

   ```bash
   swift-ring-builder container.builder \
     add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6201 \
     --device DEVICE_NAME --weight DEVICE_WEIGHT
   ```

   替换为 `STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS` 存储节点上管理网络的 IP 地址。将 `DEVICE_NAME` 替换为同一存储节点上的存储设备名称。例如，使用 Install 中的第一个存储节点，并使用 `/dev/sdb` [配置存储节点](https://docs.openstack.org/swift/2024.1/install/storage-install.html#storage) 存储设备和重量 100：

   ```bash
   swift-ring-builder container.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6201 --device sdc --weight 100
   ```

   对每个存储节点上的每个存储设备重复此命令。在示例架构中，使用四种变体的命令：

   ```bash
   swift-ring-builder container.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6201 --device sdb --weight 100
   
   swift-ring-builder container.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6201 --device sdc --weight 100
   
   swift-ring-builder container.builder add \
     --region 1 --zone 2 --ip 10.0.0.30 --port 6201 --device sdb --weight 100
   
   swift-ring-builder container.builder add \
     --region 1 --zone 2 --ip 10.0.0.30 --port 6201 --device sdc --weight 100
   ```

4. 验证环内容：

   ```bash
   swift-ring-builder container.builder
   ```

5.  重新平衡环：

   ```bash
   swift-ring-builder container.builder rebalance
   ```



##### 创建对象环

对象服务器使用对象环来维护对象位置的列表 在本地设备上。

1. 切换到 `/etc/swift` 目录。

2. 创建基本 `object.builder` 文件：

   ```bash
   swift-ring-builder object.builder create 10 2 24
   ```

3. 将每个存储节点添加到环中：

   ```bash
   swift-ring-builder object.builder \
     add --region 1 --zone 1 --ip STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS --port 6200 \
     --device DEVICE_NAME --weight DEVICE_WEIGHT
   ```

   替换为 `STORAGE_NODE_MANAGEMENT_INTERFACE_IP_ADDRESS` 存储节点上管理网络的 IP 地址。将 `DEVICE_NAME` 替换为同一存储节点上的存储设备名称。例如，使用 Install 中的第一个存储节点 [，并配置](https://docs.openstack.org/swift/2024.1/install/storage-install.html#storage)存储设备为 `/dev/sdb` 且权重为 100 的存储节点：

   ```bash
   swift-ring-builder object.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6200 --device sdc --weight 100
   ```

   对每个存储节点上的每个存储设备重复此命令。在示例架构中，使用四种变体的命令：

   ```bash
   swift-ring-builder object.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6200 --device sdb --weight 100
   
   swift-ring-builder object.builder add \
     --region 1 --zone 1 --ip 10.0.0.20 --port 6200 --device sdc --weight 100
   
   swift-ring-builder object.builder add \
     --region 1 --zone 2 --ip 10.0.0.30 --port 6200 --device sdb --weight 100
   
   swift-ring-builder object.builder add \
     --region 1 --zone 2 --ip 10.0.0.30 --port 6200 --device sdc --weight 100
   ```

4. 验证环内容：

   ```bash
   swift-ring-builder object.builder
   ```

5. 重新平衡环：

   ```bash
   swift-ring-builder object.builder rebalance
   ```

分发环配置文件

- 复制 `account.ring.gz`、`container.ring.gz` 和 `object.ring.gz` 文件到每个存储节点和运行代理服务的任何其他节点上的 `/etc/swift` 目录。

  ```bash
  scp /etc/swift/*.ring.gz compute01:/etc/swift
  scp /etc/swift/*.ring.gz compute02:/etc/swift
  ```

  

#### 完成安装

1. 从 Object Storage 源存储库获取文件：`/etc/swift/swift.conf`

   ```bash
   curl -o /etc/swift/swift.conf \
     https://opendev.org/openstack/swift/raw/branch/master/etc/swift.conf-sample
   ```

   

2. 编辑文件并完成以下作：`/etc/swift/swift.conf`

   - 在该部分中，为您的环境配置哈希路径前缀和后缀。`[swift-hash]`

     ```bash
     [swift-hash]
     ...
     swift_hash_path_suffix = HASH_PATH_SUFFIX
     swift_hash_path_prefix = HASH_PATH_PREFIX
     ```

     将 `HASH_PATH_PREFIX `和 `HASH_PATH_SUFFIX `替换为唯一值。

     可以使用`openssl`创建一个32为的随机数

     ```bash
     openssl rand -base64 32
     ```

   - 在该部分中，配置默认存储策略：`[storage-policy：0]`

     ```bash
     [storage-policy:0]
     ...
     name = Policy-0
     default = yes
     ```

3. 将文件`swift.conf`复制到每个存储节点和运行代理服务的任何其他节点上的`/etc/swift`目录。

   ```bash
   scp /etc/swift/swift.conf compute01:/etc/swift
   scp /etc/swift/swift.conf compute02:/etc/swift
   ```

4. 在所有节点上，确保配置目录的正确所有权：

   ```bash
   chown -R root:swift /etc/swift
   ```

5. 在控制器节点和运行代理服务的任何其他节点上，重新启动 Object Storage 代理服务，包括其依赖项：

   ```bash
   service memcached restart
   service swift-proxy restart
   ```

6. 在存储节点上，启动 Object Storage 服务：

   ```bash
   swift-init all start
   ```



**验证服务**

> 在控制节点上执行这些步骤

1. 获取`admin`凭据：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

2. 显示服务状态：

   ```bash
   swift stat
                           Account: AUTH_ed0b60bf607743088218b0a533d5943f
                        Containers: 0
                           Objects: 0
                             Bytes: 0
       X-Account-Project-Domain-Id: default
                       X-Timestamp: 1444143887.71539
                        X-Trans-Id: tx1396aeaf17254e94beb34-0056143bde
            X-Openstack-Request-Id: tx1396aeaf17254e94beb34-0056143bde
                      Content-Type: text/plain; charset=utf-8
                     Accept-Ranges: bytes
   ```

3. 创建 `container1` 容器：

   ```bash
   openstack container create container1
   ```

4. 将测试文件上传到 `container1` 容器：

   ```bash
   openstack object create container1 FILE
   ```

   将 `FILE` 替换为你要上传到 `container1` 容器的文件名称。

5. 列出 `container1` 容器中的文件：

   ```bash
   openstack object list container1
   +------+
   | Name |
   +------+
   | FILE |
   +------+
   ```

6. 从 `container1` 容器下载测试文件：

   ```bash
   openstack object save container1 FILE
   ```

   将 `FILE` 替换为你上传到 `container1` 容器的文件名称。



### Heat


#### 先决条件
在安装和配置 Orchestration 之前，您必须创建数据库、服务凭证和 API 终端节点。编排还需要 Identity 服务中的其他信息。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

     ```bash
     mysql -u root -p000000
     ```

   - 创建 `heat` 数据库：

     ```bash
     CREATE DATABASE heat;
     ```

   - 授予对 `heat` 数据库的适当访问权限：

     ```bash
     GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'localhost' \
       IDENTIFIED BY '000000';
     GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'%' \
       IDENTIFIED BY '000000';
     ```

     将 `HEAT_DBPASS` 替换为合适的密码。

2. 获取`管理员`凭证以获取仅限管理员的 CLI 命令的访问权限：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 要创建服务凭证，请完成以下步骤：

   - 创建 `heat` 用户：

     ```bash
     openstack user create --domain default --password-prompt heat
     ```

   - 为 `heat` 用户添加 `admin` 角色：

     ```bash
     openstack role add --project service --user heat admin
     ```

   - 创建 `heat` 和 `heat-cfn` 服务实体：

     ```bash
     openstack service create --name heat \
       --description "Orchestration" orchestration
     openstack service create --name heat-cfn \
       --description "Orchestration"  cloudformation
     ```

4. 创建 Orchestration 服务 API 端点：

   ```bash
   openstack endpoint create --region RegionOne \
     orchestration public http://controller:8004/v1/%\(tenant_id\)s
   openstack endpoint create --region RegionOne \
     orchestration internal http://controller:8004/v1/%\(tenant_id\)s
   openstack endpoint create --region RegionOne \
     orchestration admin http://controller:8004/v1/%\(tenant_id\)s
     
   openstack endpoint create --region RegionOne \
     cloudformation public http://controller:8000/v1
   openstack endpoint create --region RegionOne \
     cloudformation internal http://controller:8000/v1
   openstack endpoint create --region RegionOne \
     cloudformation admin http://controller:8000/v1
   ```

5. 编排需要 Identity 服务中的其他信息来管理堆栈。要添加此信息，请完成以下步骤：

   - 创建包含堆栈的项目和用户的`热`域：

     ```bash
     openstack domain create --description "Stack projects and users" heat
     ```

   - 创建 `heat_domain_admin` 用户以管理 `heat` 域中的项目和用户：

     ```bash
     openstack user create --domain heat --password-prompt heat_domain_admin
     ```

   - 将 `admin` 角色添加到 `heat_domain_admin` 中的 `heat` 域，以启用 `heat_domain_admin` 用户的管理堆栈管理权限：

     ```bash
     openstack role add --domain heat --user-domain heat --user heat_domain_admin admin
     ```

   - 创建 `heat_stack_owner` 角色：

     ```bash
     openstack role create heat_stack_owner
     ```

   - 为 `demo` 工程和用户添加 `heat_stack_owner` 角色，开启 `demo` 用户对堆栈的管理。

     ```bash
     openstack role add --project demo --user demo heat_stack_owner
     # 实际
     openstack role add --project "Development Department" --user developer heat_stack_owner
     ```

   - 创建 `heat_stack_user` 角色：

     ```bash
     openstack role create heat_stack_user
     ```

#### 安装和配置组件

1. 安装软件包：

   ```bash
   apt-get install heat-api heat-api-cfn heat-engine -y 
   ```

2. 编辑 `/etc/heat/heat.conf` 文件并完成以下作：

   - 在 `[database]` 部分中，配置数据库访问：

     ```bash
     [database]
     connection = mysql+pymysql://heat:000000@controller/heat
     ```

     将 `HEAT_DBPASS` 替换为您为 Orchestration 数据库选择的密码

   - 在 `[DEFAULT]` 部分中，配置 `RabbitMQ` 消息队列访问：

     ```bash
     [DEFAULT]
     transport_url = rabbit://openstack:000000@controller
     ```

     将 `RABBIT_PASS` 替换为您为 `openstack` 帐户。

   - 在 `[keystone_authtoken]` 中，`[受托人]` 和 `[clients_keystone]` 部分中，配置 Identity Service 访问权限：

     ```bash
     [keystone_authtoken]
     www_authenticate_uri = http://controller:5000
     auth_url = http://controller:5000
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = default
     user_domain_name = default
     project_name = service
     username = heat
     password = 000000
     
     [trustee]
     ...
     auth_type = password
     auth_url = http://controller:5000
     username = heat
     password = 000000
     user_domain_name = default
     
     [clients_keystone]
     ...
     auth_uri = http://controller:5000
     ```

     将 `HEAT_PASS` 替换为您为 `heat` 用户。

   - 在 `[DEFAULT]` 部分中，配置元数据和等待条件 URL：

     ```bash
     [DEFAULT]
     ...
     heat_metadata_server_url = http://controller:8000
     heat_waitcondition_server_url = http://controller:8000/v1/waitcondition
     ```

   - 在 `[DEFAULT]` 部分中，配置堆栈域和管理凭证：

     ```bash
     [DEFAULT]
     ...
     stack_domain_admin = heat_domain_admin
     stack_domain_admin_password = 000000
     stack_user_domain_name = heat
     ```

     将 `HEAT_DOMAIN_PASS` 替换为您为 `heat_domain_admin` Identity 服务中的用户设置的密码。

3. 填充 Orchestration 数据库：

   ```bash
   su -s /bin/sh -c "heat-manage db_sync" heat
   ```

#### 完成安装

重新启动 Orchestration 服务：

```bash
service heat-api restart
service heat-api-cfn restart
service heat-engine restart
```

#### 验证

```bash
root@controller:~# openstack orchestration service list
+------------+-------------+---------------------+------------+--------+---------------------+--------+
| Hostname   | Binary      | Engine ID           | Host       | Topic  | Updated At          | Status |
+------------+-------------+---------------------+------------+--------+---------------------+--------+
| controller | heat-engine | 29e876e1-9ddc-46e3- | controller | engine | 2025-06-            | up     |
|            |             | a696-0e4e476793ef   |            |        | 06T11:42:23.000000  |        |
| controller | heat-engine | 59f58adc-321f-4730- | controller | engine | 2025-06-            | up     |
|            |             | 82ab-918d5565527e   |            |        | 06T11:42:23.000000  |        |
| controller | heat-engine | 078b1bd6-c692-4b26- | controller | engine | 2025-06-            | up     |
|            |             | b839-5142a7dcd397   |            |        | 06T11:42:23.000000  |        |
| controller | heat-engine | 7cbd8773-5a52-41ab- | controller | engine | 2025-06-            | up     |
|            |             | b858-9bf4cf6e8616   |            |        | 06T11:42:23.000000  |        |
+------------+-------------+---------------------+------------+--------+---------------------+--------+
```

#### 测试

我们测试使用heat的模板创建启动一个实例

**创建模板**

Orchestration 服务使用模板来描述堆栈。 要了解模板语言，请参阅官网的[模板指南](https://docs.openstack.org/heat/2024.1/template_guide/index.html#template-guide)。

创建包含以下内容的文件：`test-template.yml`，把`properties`的信息更改为真实的

```bash
heat_template_version: 2015-10-15
description: Launch a basic instance with CirrOS image using the
             ``m1.tiny`` flavor, ``mykey`` key,  and one network.

parameters:
  NetID:
    type: string
    description: Network ID to use for the instance.

resources:
  server:
    type: OS::Nova::Server
    properties:
      image: cirros
      flavor: m1.tiny
      key_name: mykey
      networks:
      - network: { get_param: NetID }

outputs:
  instance_name:
    description: Name of the instance.
    value: { get_attr: [ server, name ] }
  instance_ip:
    description: IP address of the instance.
    value: { get_attr: [ server, first_address ] }
```

**创建堆栈**

使用模板`test-template.yml`创建堆栈。

1. 获取要执行的凭据

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

2. 确定可用网络

   ```bash
   openstack network list
   +--------------------------------------+--------------+--------------------------------------+
   | ID                                   | Name         | Subnets                              |
   +--------------------------------------+--------------+--------------------------------------+
   | 7aeaea63-aa03-4c94-80a3-4158d84c8120 | provider     | 1d6a6b25-218e-47d0-8a6c-5f9847ecfcaa |
   +--------------------------------------+--------------+--------------------------------------+
   ```
   
3. 设置环境变量以反映网络的 ID。 例如，使用提供商网络：`NET_ID`

   ```bash
   export NET_ID=$(openstack network list | awk '/ provider / { print $2 }')
   ```

4. 在提供商网络上创建一个 CirrOS 实例的堆栈：

   ```bash
   openstack stack create -t test-template.yml --parameter "NetID=$NET_ID" stack
   ```
   
5. 片刻之后，验证堆栈创建成功：

   ```bash
   openstack stack list
   ```
   
6.  显示实例的名称和 IP 地址，并与输出进行比较 的 OpenStack 客户端：

   ```bash
   openstack stack output show --all stack
   ```
   
   ```bash
   openstack server list
   ```
   
7. 删除堆栈。

   ```bash
   openstack stack delete --yes stack
   ```

	此时使用模板创建的实例也被随之删除~



### Skyline



skyline其实可以使用docker这种更方便的方式去部署安装，但目前暂未在手动部署的openstack环境中实验成功，在kolla-ansible这个官网本身就旨在使用容器化部署的方式中是没有问题的。所以本实验采用源码安装的方式



#### 源安装

通过git仓库中的资源安装，按照官网制作会存在mysql连接失败；根据个人看法，是由于skyline原本更适配于kolla-ansible这类使用docker容器部署的openstack集群，因为mysql连接失败的原因就是缺少了一个mysqlclient的pip库，而这个kolla-ansible自带

##### skyline-console

###### 先决条件

在安装和配置 Skyline APIServer 服务之前，您必须创建一个数据库。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以用户`root`身份连接到数据库服务器：

     ```bash
     mysql
     ```

   - 创建数据库：`skyline`

     ```bash
     CREATE DATABASE skyline DEFAULT CHARACTER SET \
       utf8 DEFAULT COLLATE utf8_general_ci;
     ```

   - 授予对数据库`skyline`的正确访问权限：

     ```bash
     GRANT ALL PRIVILEGES ON skyline.* TO 'skyline'@'localhost' \
       IDENTIFIED BY '000000';
     GRANT ALL PRIVILEGES ON skyline.* TO 'skyline'@'%' \
       IDENTIFIED BY '000000';
     ```

2. 获取凭据以访问仅限管理员`admin`的 CLI 命令：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 要创建服务凭证，请完成以下步骤：

   - 创建用户`skyline`：

     ```bash
     openstack user create --domain default --password-prompt skyline
     ```

   - 将角色`admin`添加到用户`skyline`：

     ```bash
     openstack role add --project service --user skyline admin
     ```



##### 安装和配置组件

我们将从源代码安装 Skyline APIServer 服务。

1. Git 从 OpenDev （GitHub） 克隆存储库

   ```bash
   sudo apt update
   sudo apt install -y git
   cd ${HOME}
   git clone https://opendev.org/openstack/skyline-apiserver.git
   ```

   如果遇到以下错误，则需要运行命令：`sudo apt install -y ca-certificates`

   ```bash
   fatal: unable to access ‘https://opendev.org/openstack/skyline-apiserver.git/’: server certificate verification failed. CAfile: none CRLfile: none
   致命：无法访问“https://opendev.org/openstack/skyline-apiserver.git/”：服务器证书验证失败。CA 文件：无 CRL 文件：无
   ```

2. 从源代码安装 skyline-apiserver

   ```bash
   sudo apt install -y python3-pip
   sudo pip3 install skyline-apiserver/
   ```

   > 补充
   >
   > 如果遇到一下报错
   >
   > ```bash
   > Installing collected packages: typing-extensions, types-six, sniffio, python-multipart, loguru, immutables, h11, gunicorn, ecdsa, annotated-types, aiomysql, uvicorn, typing-inspection, SQLAlchemy, python-jose, pydantic-core, httpcore, exceptiongroup, aiosqlite, pydantic, databases, anyio, starlette, httpx, fastapi, skyline-apiserver
   >   Attempting uninstall: typing-extensions
   >     Found existing installation: typing-extensions 3.10.0.2
   >     Not uninstalling typing-extensions at /usr/lib/python3/dist-packages, outside environment /usr
   >     Can't uninstall 'typing-extensions'. No files were found to uninstall.
   >   Attempting uninstall: SQLAlchemy
   >     Found existing installation: SQLAlchemy 1.4.50
   >     Not uninstalling sqlalchemy at /usr/lib/python3/dist-packages, outside environment /usr
   >     Can't uninstall 'SQLAlchemy'. No files were found to uninstall.
   > ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
   > cinder 24.2.0 requires boto3>=1.18.49, which is not installed.
   > cinder 24.2.0 requires google-api-python-client>=1.11.0, which is not installed.
   > cinder 24.2.0 requires oauth2client>=4.1.3, which is not installed.
   > cinder 24.2.0 requires rtslib-fb>=2.1.74, which is not installed.
   > cinder 24.2.0 requires zstd>=1.4.5.1, which is not installed.
   > Successfully installed SQLAlchemy-2.0.43 aiomysql-0.2.0 aiosqlite-0.21.0 annotated-types-0.7.0 anyio-4.11.0 databases-0.9.0 ecdsa-0.19.1 exceptiongroup-1.3.0 fastapi-0.118.0 gunicorn-23.0.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 immutables-0.21 loguru-0.5.3 pydantic-2.11.10 pydantic-core-2.33.2 python-jose-3.3.0 python-multipart-0.0.20 skyline-apiserver-7.1.0.dev8 sniffio-1.3.1 starlette-0.48.0 types-six-1.17.0.20250515 typing-extensions-4.15.0 typing-inspection-0.4.2 uvicorn-0.30.1
   > WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
   > ```
   >
   > 可以通过创建一个python的虚拟环境来下载，防止被其他系统内的python库影响到
   >
   > ```bash
   > python3 -m venv /root/skyline
   > source /root/skyline/bin/activate
   > cd ~
   > sudo pip3 install skyline-apiserver/
   > ```

3. 确保已经创建了 skyline-apiserver 的一些文件夹

   ```bash
   sudo mkdir -p /etc/skyline /var/log/skyline
   ```

4. 将配置文件复制到配置文件文件夹 `/etc/skyline`

   ```bash
   sudo cp ${HOME}/skyline-apiserver/etc/gunicorn.py /etc/skyline/gunicorn.py
   sudo sed -i "s/^bind = *.*/bind = ['0.0.0.0:28000']/g" /etc/skyline/gunicorn.py
   sudo cp ${HOME}/skyline-apiserver/etc/skyline.yaml.sample /etc/skyline/skyline.yaml
   ```

   > 我们需要将 `/etc/skyline/gunicorn.py`中的值更改为``0.0.0.0：28000`  。默认值为 `unix:/var/lib/skyline/skyline.sock`。

> 更改`/etc/skyline/skyline.yaml` 中的相关配置。配置的详细介绍可以在[设置参考中找到 ](https://docs.openstack.org/skyline-apiserver/latest/configuration/settings.html#configuration-settings)。

```bash
default:
  database_url: mysql://skyline:000000@10.0.0.10:3306/skyline
  debug: true
  log_dir: /var/log/skyline
openstack:
  keystone_url: http://10.0.0.10:5000/v3/
  system_user_password: 000000
```

> 完整模板

```yaml
default:
  access_log_file: skyline-nginx-access.log
  access_token_expire: 3600
  access_token_renew: 1800
  cafile: ''
  cors_allow_origins: []
  database_url: mysql://skyline:000000@10.0.0.10:3306/skyline
  debug: false
  error_log_file: skyline-nginx-error.log
  log_dir: /var/log/skyline
  log_file: skyline.log
  policy_file_path: /etc/skyline/policy
  policy_file_suffix: policy.yaml
  prometheus_basic_auth_password: ''
  prometheus_basic_auth_user: ''
  prometheus_enable_basic_auth: false
  prometheus_endpoint: http://10.0.0.10:9091
  secret_key: '000000'
  session_name: session
  ssl_enabled: true
openstack:
  base_domains:
  - heat_user_domain
  default_region: RegionOne
  enforce_new_defaults: true
  extension_mapping:
    floating-ip-port-forwarding: neutron_port_forwarding
    fwaas_v2: neutron_firewall
    qos: neutron_qos
    vpnaas: neutron_vpn
  interface_type: public
  keystone_url: http://10.0.0.10:5000/v3/
  nginx_prefix: /api/openstack
  reclaim_instance_interval: 604800
  service_mapping:
    baremetal: ironic
    block-storage: cinder 
    compute: nova
    container: zun
    container-infra: magnum
    database: trove
    dns: designate
    identity: keystone
    image: glance
    instance-ha: masakari
    key-manager: barbican
    load-balancer: octavia
    network: neutron
    object-store: swift
    orchestration: heat
    placement: placement
    sharev2: manilav2
  sso_enabled: false
  sso_protocols:
  - openid
  sso_region: RegionOne
  system_admin_roles:
  - admin
  - system_admin
  system_project: service
  system_project_domain: Default
  system_reader_roles:
  - system_reader
  system_user_domain: Default
  system_user_name: skyline
  system_user_password: '000000'
  user_default_domain: Default
setting:
  base_settings:
  - flavor_families
  - gpu_models
  - usb_models
  flavor_families:
  - architecture: x86_architecture
    categories:
    - name: general_purpose
      properties: []
    - name: compute_optimized
      properties: []
    - name: memory_optimized
      properties: []
    - name: high_clock_speed
      properties: []
  - architecture: heterogeneous_computing
    categories:
    - name: compute_optimized_type_with_gpu
      properties: []
    - name: visualization_compute_optimized_type_with_gpu
      properties: []
  gpu_models:
  - nvidia_t4
  usb_models:
  - usb_c
```

5. 填充 Skyline APIServer 数据库

   ```bash
   cd ${HOME}/skyline-apiserver/
   make db_sync
   ```



##### 完成安装

设置启动服务配置 `/etc/systemd/system/skyline-apiserver.service`

```bash
[Unit]
Description=Skyline APIServer

[Service]
Type=simple
ExecStart=/usr/local/bin/gunicorn -c /etc/skyline/gunicorn.py skyline_apiserver.main:app
LimitNOFILE=32768

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl enable skyline-apiserver
sudo systemctl start skyline-apiserver
```



##### skyline-apiserver



###### 先决条件

如果您使用 Ubuntu 20.04 64 位或 Ubuntu 22.04 64 位

```bash
sudo apt update -y
sudo apt install git python3-pip nginx make ssl-cert -y
sudo apt install libgtk2.0-0 libgtk-3-0 libgbm-dev libnotify-dev libgconf-2-4 libnss3 libxss1 libasound2 libxtst6 xauth xvfb -y
```

1. 安装 nvm（Node.js 版本控制系统）

   ```bash
   wget -P /root/ --tries=10 --retry-connrefused --waitretry=60 --no-dns-cache --no-cache  https://raw.githubusercontent.com/nvm-sh/nvm/master/install.sh
   bash /root/install.sh
   . /root/.nvm/nvm.sh
   ```

2. 安装 nodejs

   ```bash
   nvm install --lts=gallium
   nvm alias default lts/gallium
   nvm use default
   ```

3. 检查版本node和npm

   ```bash
   node -v
   出现 v16.*.*即为成功
   ```

   ```bash
   npm -v
   出现8.*.*即为成功
   ```

4. 安装yarn

   ```bash
   npm install -g yarn
   ```



###### 安装和配置组件

我们将从源代码安装 Skyline 控制台服务。

1. Git 从 OpenDev （GitHub） 克隆存储库

   ```bash
   cd /root
   git clone https://opendev.org/openstack/skyline-console.git
   ```

2. 安装 skyline-console

   ```bash
   cd /root/skyline-console
   make package
   sudo python3 -m pip install --force-reinstall dist/skyline_console-*.whl
   ```

3. 确保已创建skyline文件夹

   ```bash
   sudo mkdir -p /etc/skyline /var/log/skyline
   ```

   > 确保 skyline.yaml 文件在 /etc/skyline 文件夹中可用

4. 生成 nginx 配置文件

   ```bash
   skyline-nginx-generator -o /etc/nginx/nginx.conf
   sudo sed -i "s/server .* fail_timeout=0;/server 0.0.0.0:28000 fail_timeout=0;/g" /etc/nginx/nginx.conf
   ```



###### 完成安装

```bash
sudo systemctl start nginx.service
sudo systemctl enable nginx.service
```



##### 关于部署上的补漏

以上就是官网全过程，但实际上还差了一步，就如一开始所说的，我们还差了一个python库 `python-mysqlclient`，目的是让skyline-apiserver连接openstack集群使用的mariadb



如果你刚按照前面的步骤操作完，可以使用`systemctl status skyline-apiserver`命令查看组件状态，会发现并未有完全启动

安装缺少的包和库

```bash
# libmariadb-dev 包含了编译mysqlclient所需要的mariadb C头文件和库
# gcc C语言编译器，用于编译过程
apt-get install -y libmariadb-dev gcc 
pip install mysqlclient
```

重启`skyline-apiserver`服务

```bash
systemctl restart skyline-apiserver
systemctl status skyline-apiserver
```



##### 关于skyline功能出错的修护

###### cinder

在使用上述方式安装了skyline之后，我遇到了一个问题。集群中的cinder服务，不显示在存储选项卡中。在经历了一系列的排查修改之后，发现需要进行两个步骤进行修复。

- 修改`/etc/skyline/skyline.yaml`文件中有关块存储的mapping值；

  将`block-storage: cinder`改为`volumev3: cinder`

  ```diff
  openstack:
    service_mapping:	
  -   block-storage: cinder
  +   volumev3: cinder
  ```

  修改了这个文件后，需要重启`skyline-apiserver`服务生效

  ```bash
  systemctl restart skyline-apiserver.service
  ```

  然后就可以看到存储选项卡中有了块存储的三个次选项卡，但还是无法进行创建

  

- 在`/etc/nginx/nginx.conf`文件中添加有关cinder的`location`块

  文件中有关于`openstack`各个服务的`location`块，但不知道为什么没有属于`cinder`的`location`块，在文件的最后添加以下内容

  ```bash
  # RegionOne cinder
          location /api/openstack/regionone/cinder/ {
              proxy_pass http://controller:8776/;
              proxy_redirect http://controller:8776/ /api/openstack/regionone/cinder/;
              proxy_buffering off;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;
              proxy_set_header X-Forwarded-Host $host;
              proxy_set_header Host controller:8776;
  ```

  而关于这个`location`块的写法解读如下：

  ```bash
          location /api/openstack/regionone/cinder/ { 	
          }
  ```

  - `/api/openstack`：用于skyline通过nginx反代理到openstack的api后端
  - `regionone`：openstack的region名称
  - `cinder`：openstack的region下的服务名称

  ```bash
  proxy_pass http://controller:8776/;
  ```

  将匹配请求转发（proxy）到上游服务器 http://controller:8776/（Cinder API 默认端口）。结尾 / 表示路径追加（例如``/cinder/v3/... `转发到 `http://controller:8776/v3/...`）

  ```bash
  proxy_redirect http://controller:8776/ /api/openstack/regionone/cinder/;
  ```

  重写响应中的 Location 重定向头。如果上游返回 Location: http://controller:8776/v3/...，替换为 Location: /api/openstack/regionone/cinder/v3/...，确保客户端看到一致的代理路径，避免暴露内部 URL。

  ```bash
  proxy_buffering off;
  ```

  禁用 Nginx 缓冲区，直接流式传输上游响应到客户端。可以减少延迟，但增加 CPU 负载。

  ```bash
  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  ```

  设置 X-Forwarded-For 头，传递客户端真实 IP

  ```bash
  proxy_set_header X-Forwarded-Proto $scheme;
  ```

  传递原始协议（$scheme = http/https）。如果 Nginx 用 HTTPS 代理 HTTP 上游，确保 Cinder 知道客户端协议（用于重定向/安全）。

  ```bash
  proxy_set_header X-Forwarded-Host $host;
  ```

  传递客户端请求主机名（$host）。

  ```bash
  proxy_set_header Host controller:8776;
  ```

  覆盖 Host 头为上游主机（controller:8776）。

  



### barbican



#### 先决条件

在安装和配置密钥管理器服务之前，您必须创建数据库、服务凭证和 API 端点。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以用户 `root` 连接数据库服务器

     ```bash
     mysql
     ```

     

   - 创建数据库 `barbican`：

     ```bash
     CREATE DATABASE barbican;
     ```

     

   - 授予对数据库 `barbican`的适当访问权限：

     ```bash
     GRANT ALL PRIVILEGES ON barbican.* TO 'barbican'@'localhost' \
       IDENTIFIED BY '000000';
     GRANT ALL PRIVILEGES ON barbican.* TO 'barbican'@'%' \
       IDENTIFIED BY '000000';
     ```

   - 退出数据库访问客户端。

     ```bash
     exit;
     ```
   
2. 获取凭证以获取仅限管理员 `admin`的 CLI 命令的访问权限：

   ```bash
   $ source /etc/keystone/admin-openrc.sh
   ```

3. 要创建服务凭证，请完成以下步骤：

   - 创建用户 `barbican`：

     ```bash
     openstack user create --domain default --password-prompt barbican
     ```

   - 给用户`barbican`添加角色 `admin` ：

     ```bash
     openstack role add --project service --user barbican admin
     ```

   - 创建角色`creator`： 

     ```bash
     openstack role create creator
     ```

   - 为用户`barbican`添加角色 `creator` ：

     ```bash
     openstack role add --project service --user barbican creator
     ```

   - 创建`barbican`服务实体：

     ```bash
     openstack service create --name barbican --description "Key Manager" key-manager
     ```

     

4. 创建密钥管理器服务 API 端点：

   ```bash
   openstack endpoint create --region RegionOne \
     key-manager public http://controller:9311
   
   openstack endpoint create --region RegionOne \
     key-manager internal http://controller:9311
     
   openstack endpoint create --region RegionOne \
     key-manager admin http://controller:9311
   ```

#### 安装并配置组件

1. 安装软件包：

   ```bash
   apt-get update
   apt-get install barbican-api barbican-keystone-listener barbican-worker -y
   ```
   
   
   
2. 编辑文件`/etc/barbican/barbican.conf`并执行以下操作： 

   - 在`[DEFAULT]`部分中，配置数据库访问： 

     ```bash
     [DEFAULT]
     sql_connection = mysql+pymysql://barbican:000000@controller/barbican
     ```
     
   - 在 `[DEFAULT]` 部分中，配置消息队列`RabbitMQ`访问：
   
     ```bash
     [DEFAULT]
     transport_url = rabbit://openstack:RABBIT_PASS@controller
     ```
     
   - 在`[keystone_authtoken]`部分中，配置身份服务访问： 

     ```bash
     [keystone_authtoken]
     www_authenticate_uri = http://controller:5000
     auth_url = http://controller:5000
     memcached_servers = controller:11211
     auth_type = password
     project_domain_name = default
     user_domain_name = default
     project_name = service
     username = barbican
     password = 000000
     ```
     
   
   替换为您在身份服务为用户 `barbican` 选择的密码`BARBICAN_PASS`。
   
   > 注释掉或删除该部分`[keystone_authtoken]`中的任何其他选项。 
   
3. 填充密钥管理器服务数据库：

   如果您希望密钥管理器服务在首次启动时自动填充数据库，请在该部分中将 db_auto_create 设置为 True。默认情况下，此功能不会启用，您可以按如下方式手动填充数据库： `[DEFAULT]`

```bash
su -s /bin/sh -c "barbican-manage db upgrade" barbican
```

> 忽略此输出中的任何弃用消息。

4. Barbican 采用插件架构，允许部署人员将机密信息存储在多个不同的后端密钥库中。默认情况下，Barbican 配置为将机密信息存储在基于文件的基本密钥库中。此密钥库不适用于生产环境。

   有关受支持的插件列表以及如何配置它们的详细说明，请参阅[配置机密存储后端](https://docs.openstack.org/barbican/2024.1/install/barbican-backend.html#barbican-backend)

#### 完成安装

重新启动密钥管理器服务：

```bash
service barbican-keystone-listener restart
service barbican-worker restart
service apache2 restart
```



### Octavia



#### 先决条件

在安装和配置服务之前，您必须创建一个数据库 服务凭证和 API 终端节点

1. 创建数据库，完成以下步骤：

   - 使用数据库访问客户端以root用户身份连接到数据库服务器

     ```bash
     mysql 
     ```

   - 创建 数据库octavia`：

     ```bash
     CREATE DATABASE octavia;
     ```

   - 授予对 数据库：`octavia` 的适当访问权限

     ```bash
     GRANT ALL PRIVILEGES ON octavia.* TO 'octavia'@'localhost' \
     IDENTIFIED BY '000000';
     GRANT ALL PRIVILEGES ON octavia.* TO 'octavia'@'%' \
     IDENTIFIED BY '000000';
     ```

   - 退出数据库访问客户端。

     ```bash
     exit;
     ```
   
2. 获取凭据以获取仅限管理员的 CLI 命令的访问权限：`admin`

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

3. 要创建 Octavia 服务凭证，请完成以下步骤：

   - 创建 user：`octavia`

     ```bash
     openstack user create --domain default --password-prompt octavia
     ```
     
   - 将角色`admin`添加到用户`octavia`：

     ```bash
     openstack role add --project service --user octavia admin
     ```
   
   - 创建 octavia 服务实体：

     ```bash
     openstack service create --name octavia --description "OpenStack Octavia" load-balancer
     ```
   
4. 创建 Load-balancer 服务 API 终端节点：

   ```bash
   openstack endpoint create --region RegionOne \
     load-balancer public http://controller:9876
     
   openstack endpoint create --region RegionOne \
     load-balancer internal http://controller:9876
     
   openstack endpoint create --region RegionOne \
     load-balancer admin http://controller:9876
   ```

5. 创建 octavia-openrc 文件

   ```bash
   cat << EOF >> /etc/keystone/octavia-openrc
   export OS_PROJECT_DOMAIN_NAME=Default
   export OS_USER_DOMAIN_NAME=Default
   export OS_PROJECT_NAME=service
   export OS_USERNAME=octavia
   export OS_PASSWORD=000000
   export OS_AUTH_URL=http://controller:5000
   export OS_IDENTITY_API_VERSION=3
   export OS_IMAGE_API_VERSION=2
   export OS_VOLUME_API_VERSION=3
   EOF
   ```

   将 OCTAVIA_PASS 替换为您在 Identity 服务中为 octavia 用户选择的密码。

6. 获取凭据以获取对 octavia CLI 命令的访问权限：`octavia`

   ```bash
   source /etc/keystone/octavia-openrc
   ```

7. 创建 amphora 图像

   For creating amphora image, please refer to the [Building Octavia Amphora Images](https://docs.openstack.org/octavia/latest/admin/amphora-image-build.html).
   有关创建 Amphora 映像的信息，请参阅[构建 Octavia Amphora 映像 ](https://docs.openstack.org/octavia/latest/admin/amphora-image-build.html)。

   ```bash
   #!/bin/bash
   # 准备环境
   sudo apt update
   sudo apt install -y git python3 python3-pip qemu-utils kpartx debootstrap bridge-utils
   
   # 克隆 Octavia 仓库
   git clone https://opendev.org/openstack/octavia.git
   cd octavia/diskimage-create
   
   # 安装 Python 依赖
   pip3 install -r requirements.txt
   
   # 构建镜像
   ./diskimage-create.sh -a amd64 -i ubuntu-minimal -d jammy -s 1 -t qcow2 -o amphora-image.qcow2
   
   # 验证镜像
   ls -lh amphora-image.qcow2
   ```

7.  网络下载amphora镜像

   ```bash
   https://github.com/osism/openstack-octavia-amphora-image
   ```
   
8. 上传amphora图像

   ```bash
   openstack image create --disk-format qcow2 --container-format bare \
     --private --tag amphora \
     --file /opt/octavia-amphora-haproxy-2024.1.qcow2 amphora-x64-haproxy
   ```

9. 为 amphora 图像创建风格

   ```bash
   openstack flavor create --id 200 --vcpus 1 --ram 1024 \
     --disk 2 "amphora" --private
   ```

   

#### 安装和配置组件

1.  安装软件包：

   ```bash
   apt install octavia-api octavia-health-manager octavia-housekeeping \
     octavia-worker python3-octavia python3-octaviaclient python3-octavia-dashboard bridge-utils -y
   ```

   如果 octavia-common 和 octavia-api 软件包要求您进行配置，请选择 No （否）。

2. 创建证书

   ```bash
   apt install -y git 
   git clone https://opendev.org/openstack/octavia.git
   cd octavia/bin/
   source create_dual_intermediate_CA.sh
   sudo mkdir -p /etc/octavia/certs/private
   sudo chmod 755 /etc/octavia -R
   sudo cp -p etc/octavia/certs/server_ca.cert.pem /etc/octavia/certs
   sudo cp -p etc/octavia/certs/server_ca-chain.cert.pem /etc/octavia/certs
   sudo cp -p etc/octavia/certs/server_ca.key.pem /etc/octavia/certs/private
   sudo cp -p etc/octavia/certs/client_ca.cert.pem /etc/octavia/certs
   sudo cp -p etc/octavia/certs/client.cert-and-key.pem /etc/octavia/certs/private
   ```

   有关生产环境，请参阅 [Octavia 证书配置指南 ](https://docs.openstack.org/octavia/latest/admin/guides/certificates.html)。

3. 获取凭据以获取对 octavia CLI 命令的访问权限：`octavia`

   ```bash
   source /etc/keystone/octavia-openrc
   ```

4. 创建安全组及其规则

   ```bash
   openstack security group create lb-mgmt-sec-grp
   openstack security group rule create --protocol icmp lb-mgmt-sec-grp
   openstack security group rule create --protocol tcp --dst-port 22 lb-mgmt-sec-grp
   openstack security group rule create --protocol tcp --dst-port 9443 lb-mgmt-sec-grp
   openstack security group create lb-health-mgr-sec-grp
   openstack security group rule create --protocol udp --dst-port 5555 lb-health-mgr-sec-grp
   ```

5. 创建用于登录到 amphora 实例的密钥对

   ```bash
   openstack keypair create --public-key ~/.ssh/id_rsa.pub octavia-key
   ```

   ![image-20251012192959130](./images/ubuntu22.04%E9%83%A8%E7%BD%B2openstack-2024.01.assets/image-20251012192959130.png)

6. 为 dhclient 创建 dhclient.conf 文件

   ```bash
   cd $HOME
   sudo mkdir -m755 -p /etc/dhcp/octavia
   sudo cp /etc/dhcp/dhclient.conf /etc/dhcp/octavia
   ```

7. 在执行以下命令时，请将 BRNAME 和 MGMT_PORT_MAC 保存在记事本中以供进一步参考。

   ```bash
   OCTAVIA_MGMT_SUBNET=172.16.0.0/12
   OCTAVIA_MGMT_SUBNET_START=172.16.0.100
   OCTAVIA_MGMT_SUBNET_END=172.16.31.254
   OCTAVIA_MGMT_PORT_IP=172.16.0.2
   
   openstack network create lb-mgmt-net
   openstack subnet create --subnet-range $OCTAVIA_MGMT_SUBNET --allocation-pool \
     start=$OCTAVIA_MGMT_SUBNET_START,end=$OCTAVIA_MGMT_SUBNET_END \
     --network lb-mgmt-net lb-mgmt-subnet
   
   SUBNET_ID=$(openstack subnet show lb-mgmt-subnet -f value -c id)
   PORT_FIXED_IP="--fixed-ip subnet=$SUBNET_ID,ip-address=$OCTAVIA_MGMT_PORT_IP"
   
   MGMT_PORT_ID=$(openstack port create --security-group \
     lb-health-mgr-sec-grp --device-owner Octavia:health-mgr \
     --host=$(hostname) -c id -f value --network lb-mgmt-net \
   $PORT_FIXED_IP octavia-health-manager-listen-port)
   
   MGMT_PORT_MAC=$(openstack port show -c mac_address -f value \
   $MGMT_PORT_ID)
   
   sudo ip link add o-hm0 type veth peer name o-bhm0
   NETID=$(openstack network show lb-mgmt-net -c id -f value)
   BRNAME=brq$(echo $NETID|cut -c 1-11)
   sudo brctl addif $BRNAME o-bhm0
   sudo ip link set o-bhm0 up
   
   sudo ip link set dev o-hm0 address $MGMT_PORT_MAC
   sudo iptables -I INPUT -i o-hm0 -p udp --dport 5555 -j ACCEPT
   sudo dhclient -v o-hm0 -cf /etc/dhcp/octavia
   ```

   > 如果前面已经创建了上述所需网络时，又需要获取`SUBNET_ID`、`OCTAVIA_MGMT_PORT_IP`、`PORT_FIXED_IP`、`MGMT_PORT_ID`、`MGMT_PORT_MAC`等值，可以这样做：

   ```bash
   # 提取 SUBNET_ID
   root@controller:~/octavia# SUBNET_ID=$(openstack subnet show lb-mgmt-subnet -f value -c id)
   root@controller:~/octavia# echo $SUBNET_ID
   46e56035-7bf8-4a3f-bd55-3127a2d19632
   
   # 提取 PORT_FIXED_IP
   root@controller:~/octavia# SUBNET_ID=$(openstack subnet show lb-mgmt-subnet -f value -c id)
   root@controller:~/octavia# PORT_FIXED_IP="--fixed-ip subnet=$SUBNET_ID,ip-address=$OCTAVIA_MGMT_PORT_IP"
   root@controller:~/octavia# echo $PORT_FIXED_IP
   --fixed-ip subnet=46e56035-7bf8-4a3f-bd55-3127a2d19632,ip-address=172.16.0.2
   
   # 提取 MGMT_PORT_ID
   root@controller:~/octavia# MGMT_PORT_ID=$(openstack port list --network lb-mgmt-net --device-owner Octavia:health-mgr -f value -c id | head -1)
   root@controller:~/octavia# echo $MGMT_PORT_ID
   c9bf44fb-c135-489d-a3b3-e99c48c2e975
   
   # 提取 MGMT_PORT_MAC
   root@controller:~/octavia# MGMT_PORT_MAC=$(openstack port show $MGMT_PORT_ID -c mac_address -f value)
   root@controller:~/octavia# echo $MGMT_PORT_MAC
   fa:16:3e:d2:e3:44
   ```

   自动脚本

   ```bash
   #!/bin/bash
   
   # 配置
   OCTAVIA_MGMT_PORT_IP=172.16.0.2 # 根据创建时所有的IP设置
   NETWORK_NAME=lb-mgmt-net
   SUBNET_NAME=lb-mgmt-subnet
   PORT_NAME=octavia-health-manager-listen-port  # 端口名称
   
   # 提取 SUBNET_ID
   SUBNET_ID=$(openstack subnet show $SUBNET_NAME -f value -c id)
   echo "SUBNET_ID: $SUBNET_ID"
   
   # 构建 PORT_FIXED_IP
   PORT_FIXED_IP="--fixed-ip subnet=$SUBNET_ID,ip-address=$OCTAVIA_MGMT_PORT_IP"
   echo "PORT_FIXED_IP: $PORT_FIXED_IP"
   
   # 提取 MGMT_PORT_ID
   MGMT_PORT_ID=$(openstack port show $PORT_NAME -f value -c id 2>/dev/null || openstack port list --network $NETWORK_NAME --device-owner Octavia:health-mgr -f value -c id | head -1)
   echo "MGMT_PORT_ID: $MGMT_PORT_ID"
   
   # 提取 MGMT_PORT_MAC
   if [ -n "$MGMT_PORT_ID" ]; then
     MGMT_PORT_MAC=$(openstack port show $MGMT_PORT_ID -c mac_address -f value)
     echo "MGMT_PORT_MAC: $MGMT_PORT_MAC"
   else
     echo "MGMT_PORT_ID not found; check port creation."
   fi
   ```

8. 创建管理端口

   ```bash
   ovs-vsctl --may-exist add-port br-int o-hm0 \
     -- set Interface o-hm0 type=internal \
     -- set Interface o-hm0 external-ids:iface-status=active \
     -- set Interface o-hm0 external-ids:attached-mac=$MGMT_PORT_MAC \
     -- set Interface o-hm0 external-ids:iface-id=$MGMT_PORT_ID
   ```

   使用以下命令检测创建是否成功

   ```bash
   # ovs-vsctl show | grep -A 5 o-hm0
           Port o-hm0
               tag: 6
               Interface o-hm0
                   type: internal
           Port tap52add46d-4a
               tag: 2
               Interface tap52add46d-4a
                   type: internal
   
   # ovs-vsctl list Interface o-hm0
   _uuid               : c8ed9c57-a360-4552-ab09-d27aad3fdbc8
   admin_state         : up
   bfd                 : {}
   bfd_status          : {}
   cfm_fault           : []
   cfm_fault_status    : []
   cfm_flap_count      : []
   cfm_health          : []
   cfm_mpid            : []
   cfm_remote_mpids    : []
   cfm_remote_opstate  : []
   duplex              : []
   error               : []
   external_ids        : {attached-mac="fa:16:3e:d2:e3:44", iface-id="c9bf44fb-c135-489d-a3b3-e99c48c2e975", iface-status=active}
   ifindex             : 55
   ingress_policing_burst: 0
   ingress_policing_kpkts_burst: 0
   ingress_policing_kpkts_rate: 0
   ingress_policing_rate: 0
   lacp_current        : []
   link_resets         : 1
   link_speed          : []
   link_state          : up
   lldp                : {}
   mac                 : []
   mac_in_use          : "72:f5:9e:17:92:1a"
   mtu                 : 1500
   mtu_request         : []
   name                : o-hm0
   ofport              : 25
   ofport_request      : []
   options             : {}
   other_config        : {}
   statistics          : {collisions=0, rx_bytes=0, rx_crc_err=0, rx_dropped=0, rx_errors=0, rx_frame_err=0, rx_missed_errors=0, rx_multicast_packets=0, rx_over_err=0, rx_packets=0, tx_bytes=3211, tx_dropped=0, tx_errors=0, tx_packets=18}
   status              : {driver_name=openvswitch}
   type                : internal
   
   
   ovs-vsctl get Interface o-hm0 type  # 应输出 "internal"
   ovs-vsctl get Interface o-hm0 external_ids:iface-status  # 应输出 "active"
   ovs-vsctl get Interface o-hm0 external_ids:attached-mac  # 应输出 $MGMT_PORT_MAC
   ovs-vsctl get Interface o-hm0 external_ids:iface-id  # 应输出 $MGMT_PORT_ID
   ```

    创建接口

   ```bash
   root@controller:~/octavia# ip link set dev o-hm0 address $MGMT_PORT_MAC
   ip link set dev o-hm0 up
   root@controller:~/octavia# iptables -I INPUT -i o-hm0 -p udp --dport 5555 -j ACCEPT
   root@controller:~/octavia# dhclient -v o-hm0 -cf /etc/dhcp/octavia
   Internet Systems Consortium DHCP Client 4.4.1
   Copyright 2004-2018 Internet Systems Consortium.
   All rights reserved.
   For info, please visit https://www.isc.org/software/dhcp/
   
   Listening on LPF/o-hm0/fa:16:3e:d2:e3:44
   Sending on   LPF/o-hm0/fa:16:3e:d2:e3:44
   Sending on   Socket/fallback
   DHCPDISCOVER on o-hm0 to 255.255.255.255 port 67 interval 3 (xid=0xd868cf6a)
   DHCPOFFER of 172.16.0.2 from 172.16.0.100
   DHCPREQUEST for 172.16.0.2 on o-hm0 to 255.255.255.255 port 67 (xid=0x6acf68d8)
   DHCPACK of 172.16.0.2 from 172.16.0.100 (xid=0xd868cf6a)
   RTNETLINK answers: File exists
   bound to 172.16.0.2 -- renewal in 32550 seconds.
   
   root@controller:~/octavia# ip a s dev o-hm0
   55: o-hm0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000
       link/ether fa:16:3e:d2:e3:44 brd ff:ff:ff:ff:ff:ff
       inet 172.16.0.2/12 metric 1024 brd 172.31.255.255 scope global dynamic o-hm0
          valid_lft 86300sec preferred_lft 86300sec
       inet6 fe80::70f5:9eff:fe17:921a/64 scope link
          valid_lft forever preferred_lft forever
   ```

9. 主机重启后，需要进行以下设置才能创建 veth 对

   编辑 `/etc/systemd/network/o-hm0.network` 文件

   ```bash
   [Match]
   Name=o-hm0
   
   [Network]
   DHCP=yes
   ```

   编辑 `/etc/systemd/system/octavia-interface.service` 文件

   ```bash
   
   [Unit]
   Description=Octavia Interface Start
   After=network.target openvswitch-switch.service
   Wants=openvswitch-switch.service
   
   [Service]
   Type=oneshot
   ExecStart=/bin/sh /opt/octavia-interface-start.sh
   RemainAfterExit=true
   StandardOutput=journal
   StandardError=journal
   
   [Install]
   WantedBy=multi-user.target
   ```

   编辑 `/opt/octavia-interface.sh` 文件

   ```bash
   #!/bin/bash
   
   set -x
   
   MAC="fa:16:3e:d2:e3:44"
   PORT_ID="c9bf44fb-c135-489d-a3b3-e99c48c2e975"
   
   sleep 120s
   
   ovs-vsctl --may-exist add-port br-int o-hm0 \
     -- set Interface o-hm0 type=internal \
     -- set Interface o-hm0 external-ids:iface-status=active \
     -- set Interface o-hm0 external-ids:attached-mac=$MAC \
     -- set Interface o-hm0 external-ids:iface-id=$PORT_ID
   
   
   ip link set dev o-hm0 address $MAC
   ip link set dev o-hm0 up
   iptables -I INPUT -i o-hm0 -p udp --dport 5555 -j ACCEPT
   
   dhclient -v o-hm0 -cf /etc/dhcp/octavia
   route del default gw 172.16.0.1
   #echo 'nameserver 192.168.35.2' > /etc/resolv.conf
   ```

      

10. 编辑文件 `/etc/octavia/octavia.conf`

    - 在`[database]`部分中，配置数据库传输URL：

      ```bash
      [database]
      connection = mysql+pymysql://octavia:000000@controller/octavia
      ```

      将 OCTAVIA_DBPASS 替换为您为 Octavia 数据库选择的密码。

    - 在`[DEFAULT]`部分中，配置 RabbitMQ 消息代理的传输 URL。

      ```bash
      [DEFAULT]
      transport_url = rabbit://openstack:000000@controller
      ```

    - 在`[oslo_messaging]`部分中，配置 RabbitMQ 消息代理的传输 URL 和主题名称。

      ```bash
      [oslo_messaging]
      topic = octavia_prov
      ```
      
    - 在`[api_settings]`部分中，配置要绑定到的主机 IP 和端口。
    
      ```bash
      [api_settings]
      bind_host = 0.0.0.0
      bind_port = 9876
      ```
    
    - 在`[keystone_authtoken]`部分中，配置身份服务访问。
    
      ```bash
      [keystone_authtoken]
      www_authenticate_uri = http://controller:5000
      auth_url = http://controller:5000
      memcached_servers = controller:11211
      auth_type = password
      project_domain_name = Default
      user_domain_name = Default
      project_name = service
      username = octavia
      password = 000000
      ```
    
      将 password的值 替换为您在 Identity 服务中为 octavia 用户选择的密码。
    
    - 在`[service_auth]`部分中，配置使用其他 OpenStack 服务的凭证 
    
      ```bash
      [service_auth]
      auth_url = http://controller:5000
      memcached_servers = controller:11211
      auth_type = password
      project_domain_name = Default
      user_domain_name = Default
      project_name = service
      username = octavia
      password = 000000
      ```
    
      将 OCTAVIA_PASS 替换为您在 Identity 服务中为 octavia 用户选择的密码。
    
    - 在 `[certificates]` 部分，配置 CA 证书的绝对路径、用于签名的私钥和密码。
    
      ```bash
      [certificates]
      server_certs_key_passphrase = insecure-key-do-not-use-this-key
      ca_private_key_passphrase = not-secure-passphrase
      ca_private_key = /etc/octavia/certs/private/server_ca.key.pem
      ca_certificate = /etc/octavia/certs/server_ca.cert.pem
      ```
    
    - 在`[haproxy_amphora]`部分中，配置客户端证书和 CA。
    
      ```bash
      [haproxy_amphora]
      server_ca = /etc/octavia/certs/server_ca-chain.cert.pem
      client_cert = /etc/octavia/certs/private/client.cert-and-key.pem
      ```
      
    - 在`[health_manager]`部分中，配置检测信号的 IP 和端口号。
    
      ```bash
      [health_manager]
      bind_port = 5555
      bind_ip = 172.16.0.2
      controller_ip_port_list = 172.16.0.2:5555
      ```
      
    - 在`[controller_worker]`部分中，配置 worker 设置。
    
      ```bash
      [controller_worker]
      amp_image_owner_id = 81dea4257b404d27a7c021e1519c633d
      amp_image_tag = amphora
      amp_ssh_key_name = octavia_ssh_key
      amp_secgroup_list = b3eccfb0-0ef5-4d2a-aa8d-e06d52892e05
      amp_boot_network_list = 42b297fd-a655-4d7f-90b5-081c9f1bf324
      amp_flavor_id = 200
      network_driver = allowed_address_pairs_driver
      compute_driver = compute_nova_driver
      amphora_driver = amphora_haproxy_rest_driver
      client_ca = /etc/octavia/certs/client_ca.cert.pem
      ```
      
      - `amp_image_owner_id`: openstack project show service -c id -f value
      
      - `amp_secgroup_list`: openstack security group show lb-mgmt-sec-grp -c id -f value
      - `amp_boot_network_list`: openstack network show lb-mgmt-net -c id -f value
    
12. 填充 octavia 数据库：

    ```bash
    octavia-db-manage --config-file /etc/octavia/octavia.conf upgrade head
    ```



#### 完成安装

重新启动服务：

```bash
# systemctl restart octavia-api octavia-health-manager octavia-housekeeping octavia-worker
```



### Magnum



#### 先决条件

在安装和配置容器基础设施管理服务之前，必须创建数据库、服务凭证和 API 端点。

1. 要创建数据库，请完成以下步骤：

   - 使用数据库访问客户端以 `root` 用户身份连接到数据库服务器：

     ```bash
     mysql
     ```

   - 创建 `magnum` 数据库：

     ```bash
     CREATE DATABASE magnum;
     ```

   - 授予对 `magnum` 数据库的正确访问权限：

     ```bash
     GRANT ALL PRIVILEGES ON magnum.* TO 'magnum'@'localhost' \
       IDENTIFIED BY '000000';
     GRANT ALL PRIVILEGES ON magnum.* TO 'magnum'@'%' \
       IDENTIFIED BY '000000';
     ```

     将 `MAGNUM_DBPASS` 替换为合适的密码。

   - 退出数据库访问客户端。

     ```bash
     exit;
     ```

     

   

2. 获取`管理员`凭据以访问仅限管理员的 CLI 命令：

   ```bash
   source /etc/keystone/admin-openrc.sh
   ```

   

3. 要创建服务凭证，请完成以下步骤：

   - 创建 `magnum` 用户：

   ```bash
   openstack user create --domain default \
     --password-prompt magnum
   ```

   - 将`管理员`角色添加到 `magnum` 用户：

     ```bash
     openstack role add --project service --user magnum admin
     ```

   - 创建 `magnum` 服务实体：

     ```bash
     openstack service create --name magnum \
       --description "OpenStack Container Infrastructure Management Service" \
       container-infra
     ```

     

4. 创建容器基础架构管理服务 API 端点：

   ```bash
   openstack endpoint create --region RegionOne \
     container-infra public http://controller:9511/v1
   openstack endpoint create --region RegionOne \
     container-infra internal http://controller:9511/v1
   openstack endpoint create --region RegionOne \
     container-infra admin http://controller:9511/v1
   ```

   将 `CONTROLLER_IP` 替换为 magnum 监听的 IP。或者，您可以使用计算实例可访问的主机名。

   

5. Magnum 需要标识服务中的其他信息来管理 COE 群集。要添加此信息，请完成以下步骤：

   - 创建包含项目和用户的 `magnum` 域：

     ```bash
     openstack domain create --description "Owns users and projects \
       created by magnum" magnum
     ```

   - 创建 `magnum_domain_admin` 用户来管理 `magnum` 域中的项目和用户：

     ```bash
     openstack user create --domain magnum --password-prompt \
       magnum_domain_admin
     ```

   - 将`管理员`角色 `magnum_domain_admin` 添加到 `Magnum` 域，以启用 `magnum_domain_admin` 用户的管理管理权限：

     ```bash
     openstack role add --domain magnum --user-domain magnum --user \
       magnum_domain_admin admin
     ```

     

#### 安装和配置组件

1. 安装通用包和库包：

   ```bash
   DEBIAN_FRONTEND=noninteractive apt-get install magnum-api magnum-conductor python3-magnumclient
   ```

2. 编辑 `/etc/magnum/magnum.conf` 文件：

   - 在 `[api]` 部分，配置主机：

     ```bash
     [api]
     host = 10.0.0.10
     ```

     将 `CONTROLLER_IP` 替换为您希望 magnum api 监听的 IP 地址。

   - 在 `[certificates]` 部分中，选择 `barbican`（如果您没有安装 barbican，则选择 `x509keypair`）：

     - 使用 barbican 存储证书：

       ```bash
       [certificates]
       cert_manager_type = barbican
       ```

     - 要将 x509 证书存储在 magnum 的数据库中：

       ```bash
       [certificates]
       cert_manager_type = x509keypair
       ```

     - 在 `[cinder_client]` 部分中，配置区域名称：

       ```bash
       [cinder_client]
       region_name = RegionOne
       ```

     - 在 `[database]` 部分，配置数据库访问：

       ```bash
       [database]
       connection = mysql+pymysql://magnum:000000@controller/magnum
       ```

       将 `MAGNUM_DBPASS` 替换为您为 magnum 数据库选择的密码。

     - 在 `[keystone_authtoken]` 和 `[trust]` 部分中，配置身份服务访问：

       ```bash
       [keystone_authtoken]
       memcached_servers = controller:11211
       auth_version = v3
       www_authenticate_uri = http://controller:5000/v3
       project_domain_id = default
       project_name = service
       user_domain_id = default
       password = 000000
       username = magnum
       auth_url = http://controller:5000
       auth_type = password
       admin_user = magnum
       admin_password = 000000
       admin_tenant_name = service
       
       [trust]
       trustee_domain_name = magnum
       trustee_domain_admin_name = magnum_domain_admin
       trustee_domain_admin_password = 000000
       trustee_keystone_interface = public
       ```

       将 MAGNUM_PASS 替换为您在 magnum 用户中选择的密码 身份服务，DOMAIN_ADMIN_PASS 并使用您为 `magnum_domain_admin` 用户。

       将 KEYSTONE_INTERFACE 替换为`public`或`internal ` 取决于您的网络配置。如果您的实例无法到达 内部 Keystone 端点，这在生产中经常出现 环境，它应该设置为`public`。默认为`public`

     - 在该 `[oslo_messaging_notifications]` 部分中，配置 `driver`：

       ```bash
       [oslo_messaging_notifications]
       driver = messaging
       ```

     - 在 `【DEFAULT】` 部分，配置 `RabbitMQ` 消息队列访问：

       ```bash
       [DEFAULT]
       transport_url = rabbit://openstack:000000@controller
       ```

3. 填充 Magnum 数据库：

   ```bash
   su -s /bin/sh -c "magnum-db-manage upgrade" magnum
   ```

### 完成安装

重启容器基础架构管理服务：

```bash
service magnum-api restart
service magnum-conductor restart
```

#### 测试

#### 将集群所需的镜像上传到 Image 服务

```bash
export FCOS_VERSION="35.20220116.3.0"
wget https://builds.coreos.fedoraproject.org/prod/streams/stable/builds/${FCOS_VERSION}/x86_64/fedora-coreos-${FCOS_VERSION}-openstack.x86_64.qcow2.xz
unxz fedora-coreos-${FCOS_VERSION}-openstack.x86_64.qcow2.xz
```

```bash
root@controller:/opt# openstack image create \
                      --disk-format=qcow2 \
                      --container-format=bare \
                      --file=fedora-coreos-35.20220116.3.0-openstack.x86_64.qcow2 \
                      --property os_distro='fedora-coreos' \
                      fedora-coreos-latest
```

####  配置 Kubernetes 集群并创建部署

```bash
openstack coe cluster template create kubernetes-cluster-template \
                     --image fedora-coreos-latest \
                     --external-network provider \
                     --dns-nameserver 8.8.8.8 \
                     --master-flavor m2.fedora \
                     --flavor m2.fedora \
                     --coe kubernetes
```



### 测试Magnum-api方式创建Kubernetes集群



```bash
export OS_DISTRO=ubuntu # you can change this to "flatcar" if you want to use Flatcar
for version in v1.24.16 v1.25.12 v1.26.7 v1.27.4; do \
  [[ "${OS_DISTRO}" == "ubuntu" ]] && IMAGE_NAME="ubuntu-2204-kube-${version}" || IMAGE_NAME="flatcar-kube-${version}"; \
  curl -LO https://object-storage.public.mtl1.vexxhost.net/swift/v1/a91f106f55e64246babde7402c21b87a/magnum-capi/${IMAGE_NAME}.qcow2; \
  openstack image create ${IMAGE_NAME} --disk-format=qcow2 --container-format=bare --property os_distro=${OS_DISTRO} --file=${IMAGE_NAME}.qcow2; \
  openstack coe cluster template create \
      --image flatcar-kube-v1.24.16 \
      --external-network provider \
      --dns-nameserver 8.8.8.8 \
      --master-lb-enabled \
      --master-flavor m2.ubuntu \
      --flavor m2.ubuntu \
      --network-driver calico \
      --docker-storage-driver overlay2 \
      --coe kubernetes \
      --label kube_tag=${version} \
      k8s-${version};
done;
```

```bash
openstack coe cluster template create \
    --image flatcar-kube-v1.24.16 \
    --external-network provider \
    --dns-nameserver 8.8.8.8 \
    --master-flavor m2.ubuntu \
    --flavor m2.ubuntu \
    --network-driver fannel \
    --docker-storage-driver overlay2 \
    --coe kubernetes \
    --labels kube_tag=v1.24.16 \
    k8s-v1.24.16
```

