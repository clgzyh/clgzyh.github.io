---
layout    : post
title     : "Hadoop-3.4.2集群化部署"
date      : 2025-11-19
lastupdate: 2025-11-19
categories: Hadoop
render_with_liquid: false
cheatsheet: false
---

# Hadoop-3.4.2集群化部署



本次实验采用三节点，主节点部署 **NameNode** + **ResourceManager** 纯管理（不运行**DataNode**/**NodeManager**），从节点部署 **DataNode** + **NodeManger**



## 一、环境准备



1. 主机规划

   | 主机名 | IP地址    | 角色                                         |
   | ------ | --------- | -------------------------------------------- |
   | Master | 10.0.0.10 | NameNode、ResourceManager、SecondaryNameNode |
   | Slave1 | 10.0.0.20 | DataNode、NodeManager                        |
   | Slave2 | 10.0.0.30 | DataNode、NodeManager                        |

2. 软件依赖

   - 创建hadoop用户

   - 所有节点安装 **Java11** 环境

   - 安装 **Hadoop 3.4.2**
   - 配置 SSH 免密登录 （ master → 所有节点）
   - 关闭防火墙或开放对应端口



## 二、基础配置 (所有节点)

1. 创建用户`hadoop`，并授权免密sudo

   

   创建用户

   ```bash
   sudo useradd -m -s /bin/bash hadoop
   sudo passwd hadoop
   ```

   加入sudo组

   ```bash
   usermod -aG sudo hadoop
   ```

   ```bash
   visudo -f /etc/sudoers.d/hadoop
   
   # 输入以下内容
   hadoop ALL=(ALL) NOPASSWD:ALL
   ```

   ```bash
   su hadoop
   ```

   后续使用hadoop用户进行操作

2. 安装 Java

   

   使用apt安装

   ```bash
   sudo apt install -y openjdk-11-jdk
   ```

   验证

   ```bash
   $ java -version
   openjdk version "11.0.28" 2025-07-15
   OpenJDK Runtime Environment (build 11.0.28+6-post-Ubuntu-1ubuntu125.04.1)
   OpenJDK 64-Bit Server VM (build 11.0.28+6-post-Ubuntu-1ubuntu125.04.1, mixed mode, sharing)
   ```

   通过find命令查找JAVA_HOME目录所在

   ```bash
   $ find /usr/lib -name java
   /usr/lib/jvm/java-11-openjdk-amd64/bin/java
   ```

   设置JAVA_HOME

   ```bash
   echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
   echo 'export PATH=$PATH:$JAVA_HOME/bin' >> ~/.bashrc
   ```

   加载文件生效

   ```bash
   source ~/.bashrc
   ```

3. 下载并解压Hadoop

   ```bash
   cd ~
   wget https://archive.apache.org/dist/hadoop/core/hadoop-3.4.2/hadoop-3.4.2.tar.gz
   tar -zxvf hadoop-3.4.2.tar.gz
   mv hadoop-3.4.2 hadoop
   ```

   设置 Hadoop 环境变量（~/.bashrc）

   ```bash
   vim ~/.bashrc
   ```

   ```bash
   export HADOOP_HOME=/home/hadoop/hadoop
   export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
   ```

   ```bash
   source ~/.bashrc
   ```

   

## 三、配置 SSH 免密登录（仅在 master 上操作）

确保/etc/hosts有正确的条目

```bash
$ cat /etc/hosts
10.0.0.10 master
10.0.0.20 slave1
10.0.0.30 slave2
```

在master节点执行

```bash
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
ssh-copy-id hadoop@master
ssh-copy-id hadoop@slave1
ssh-copy-id hadoop@slave2
```

测试ssh免密

```bash
ssh slave1 
```



## 四、Hadoop 配置文件（在 master 节点上编辑）

进入 `$HADOOP_HOME/etc/hadoop` 目录，修改以下文件：

### 1. core-site.xml

```bash
vim core-site.xml
```

```bash
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://master:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/hadoop/tmp</value>
    </property>
</configuration>
```

### 2. hdfs-site.xml

```bash
vim hdfs-site.xml
```

```bash
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value> <!-- 根据 DataNode 数量调整 -->
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop/hadoop/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/home/hadoop/hadoop/hdfs/datanode</value>
    </property>
</configuration>
```

### 3. yarn-site.xml

```bash
vim yarn-site.xml
```

```bash
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
</configuration>
```

### 4. mapred-site.xml

```bash
vim mapred-site.xml
```

```bash
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration>
```

### 5. workers（旧版为 slaves）

```bash
vim workers
```

```bash
slave1
slave2
```

### 6. hadoop-env.sh（修改JAVA_HOME）

```bash
sed -i "s|# export JAVA_HOME=.*|export JAVA_HOME=$JAVA_HOME|" "$HADOOP_CONF_DIR/hadoop-env.sh"
```



## 五、分发配置到其他节点

```bash
scp -r ~/hadoop hadoop@slave1:~/
scp -r ~/hadoop hadoop@slave2:~/
```

如果前面给从节点也上传了压缩包并解压了，则分发主要配置就行

```bash
for node in slave1 slave2; do
  echo "Syncing config to $node..."
  scp ~/hadoop/etc/hadoop/{core-site.xml,hdfs-site.xml,yarn-site.xml,mapred-site.xml,workers} hadoop@$node:~/hadoop/etc/hadoop/
done
```



> **注意：**在 slave1/slave2 上同样配置 `.bashrc` 中的环境变量。
>
> ```bash
> $ tail -5 ~/.bashrc
> export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
> export PATH=$PATH:$JAVA_HOME/bin
> export HADOOP_HOME=/home/hadoop/hadoop
> export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
> export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
> ```



##  六、格式化 NameNode 并启动集群

### 1. 格式化 HDFS（仅首次）

```bash
hdfs namenode -format
```

### 2. 启动 HDFS

```bash
start-dfs.sh
```

### 3. 启动 YARN

```bash
start-yarn.sh
```

### 4. 验证进程

- master 上应有：NameNode、ResourceManager
- slave1/slave2 上应有：DataNode、NodeManager

使用 `jps` 命令查看。

Master

```bash
$ jps
1800 ResourceManager
1386 NameNode
2107 Jps
1597 SecondaryNameNode
```

Slave

```bash
$ jps
1568 Jps
1312 DataNode
1435 NodeManager
```



### 5. Web UI 访问

- HDFS: http://master:9870

  ![img](../assets/img/hadoop/image-9870.png)

- YARN: http://master:8088

  ![img](../assets/img/hadoop/image-8088-cluster.png)



## 七、运行测试任务

```bash
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.2.jar pi 2 10
```



